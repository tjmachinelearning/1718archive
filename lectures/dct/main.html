<h1 id="introduction">Introduction</h1>
<p>Decision trees are powerful and interpretable classifiers that mirror human decisions unlike many other classifiers in supervised machine learning and are the building blocks of random forests.</p>
<h1 id="definition">Definition</h1>
<p>In essence, decision trees asks a series of true/false questions to narrow down what class a particular sample is. Here is an example of a decision tree one might use in real life to decide upon an activity on a given day:</p>
<div class="figure">
<img src="dtree.jpg" alt="Real Life Decision Tree" />
<p class="caption">Real Life Decision Tree<span data-label="fig:decisiontree"></span></p>
</div>
<p>Although this figure asks categorical variable-based questions, we can ask numerical-based questions like <span class="math inline">\(``x_1 &lt; 5?&quot;\)</span> when the features are continuous. To build our tree, we start at the root node and ask a question that splits the data based on the feature such that the information gain is maximized. We continuously do this for each node until the decision tree can classify all the training data. Note that in practice this leads to overfitting, so the tree is usually pruned, i.e. a limit on the depth of the tree is set.</p>
<h2 id="information-gain">Information Gain</h2>
<p>We split each node on the feature that yields the most information gain. The formula for information gain in a binary decision tree is as follows:</p>
<p><span class="math display">\[IG(D_p, f) = I(D_p) - \frac{N_{left}}{N_p}I(D_{left}) - \frac{N_{right}}{N_p}I(D_{right})\]</span></p>
<p><span class="math inline">\(D_p\)</span> is the dataset of the parent node (the node which we are splitting), <span class="math inline">\(f\)</span> is the feature of the dataset which we are splitting on, <span class="math inline">\(N_p\)</span> is the total number of samples in the parent node, <span class="math inline">\(N_{left}\)</span> and <span class="math inline">\(N_{right}\)</span> are the number of samples in the datasets of the left child node and right child node respectively, and <span class="math inline">\(I\)</span> is the impurity measure. A node is pure if all samples in its dataset belong to the same class and is most impure when an equal number of samples belong to each class. Essentially, information gain calculates the difference between the impurity of the parent node and the impurity of the child nodes.</p>
<p>One commonly used measure of impurity is Gini impurity:</p>
<p><span class="math display">\[I_G(i) = 1 - \sum_{k=1}^{c} p(k|i)^2\]</span></p>
<p><span class="math inline">\(p(k|i)\)</span> is the proportion of samples of class <span class="math inline">\(k\)</span> to the total number of samples in the dataset of the <span class="math inline">\(i^{th}\)</span> node. The impurity is maximized when the classes of the node are perfectly mixed (for this example, consider a situation in which there are 2 classes, meaning c = 2): <span class="math display">\[1 - \sum_{k=1}^{c} 0.5^2 = 0.5\]</span></p>
<p>An alternative impurity measure is entropy, which is defined as:</p>
<p><span class="math display">\[-\sum_{k=1}^{c} p(k|i)log_2{p(k|i)}\]</span></p>
<p>Note that this function has a maximum of 1.0, not 0.5. In practice, Gini impurity and entropy yield similar results, so it is more useful to test different pruning cut-offs rather than to evaluate trees with different impurity criteria.</p>
<p>To decide on a split for a specific node, we will search for the feature and the threshold (e.g. â€œpetal length <span class="math inline">\(&lt;\)</span> 2.45 cm&quot; for a flower classifier) that maximizes the information gain. We can choose the best threshold for a feature from the feature values in the training data or from the averages of every pair of feature values in the training set. Another method is to select the best threshold from the quartiles (20%, 40%, 60%, and 80&amp; values) of the feature set.</p>
<p>Here is the pseudocode for determining the best split:</p>
<p><span class="math inline">\(\textit{IG} \gets 0\)</span> <span class="math inline">\(\textit{pot\_left, pot\_right} \gets \text{split}\textit{(parent, feature, threshold)}\)</span> <span class="math inline">\(\textit{pot\_ig} \gets \text{information\_gain}\textit{(parent, pot\_left, pot\_right)}\)</span> <span class="math inline">\(\textit{left} \gets \textit{pot\_left}\)</span> <span class="math inline">\(\textit{right} \gets \textit{pot\_right}\)</span> <span class="math inline">\(\textit{IG} \gets \textit{pot\_ig}\)</span> <em>left, right</em></p>
<p><span class="math inline">\(\text{Initialize }\textit{left}\text{ and }\textit{right}\text{ lists}\)</span> append data point to <em>left</em> append data point to <em>right</em> <em>left, right</em></p>
<p><span class="math inline">\(\textit{sum} \gets 0\)</span> <span class="math inline">\(\textit{ratio} \gets \text{(number of class labels } \textit{c}\text{)} / \text{size of dataset}\)</span> <span class="math inline">\(\textit{sum} \gets \textit{sum} + \textit{ratio}*\textit{ratio}\)</span> <em>sum</em></p>
<h1 id="practice-problems">Practice Problems</h1>
<p>Consider the following dataset:</p>
<table>
<thead>
<tr class="header">
<th align="center">x1</th>
<th align="center">x2</th>
<th align="center">x3</th>
<th align="center">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">-1</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">+1</td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">+1</td>
</tr>
<tr class="even">
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">-1</td>
</tr>
</tbody>
</table>
<ol>
<li><p>What feature will we split on at the root of our decision tree, and what will our information gain be from splitting on that feature using the Gini impurity measure?</p></li>
<li><p>Build a decision tree using the dataset. What is the depth of the tree?</p></li>
<li><p>What will the decision tree classify a data point with the features x1 = 0, x2 = 0, and x3 = 0 as (y = -1 or y = +1)?</p></li>
</ol>
<table>
<thead>
<tr class="header">
<th align="center">x1</th>
<th align="center">x2</th>
<th align="center">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">0</td>
<td align="center">+1</td>
</tr>
<tr class="even">
<td align="center">0</td>
<td align="center">1</td>
<td align="center">+1</td>
</tr>
<tr class="odd">
<td align="center">1</td>
<td align="center">0</td>
<td align="center">+1</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">1</td>
<td align="center">-1</td>
</tr>
</tbody>
</table>
<ol>
<li><p>What will the information gain be after the first split in the above data set with the Gini impurity measure? With entropy as the impurity measure?</p></li>
<li><p>What is the depth of the final decision tree?</p></li>
</ol>
