<h1 id="introduction">Introduction</h1>
<p>Decision trees are powerful and interpretable classifiers that mirror human decisions unlike many other classifiers in supervised machine learning and are the building blocks of random forests.</p>
<h1 id="definition">Definition</h1>
<p>In essence, decision trees asks a series of true/false questions to narrow down what class a particular sample is. Here is an example of a decision tree one might use in real life to decide upon an activity on a given day:</p>
<div class="figure">
<img src="dtree.jpg" alt="Real Life Decision Tree" />
<p class="caption">Real Life Decision Tree<span data-label="fig:decisiontree"></span></p>
</div>
<p>Although this figure asks categorical variable-based questions, we can ask numerical-based questions like <span class="math inline">\(&quot;x_1 &lt; 5?&quot;\)</span> when the features are continuous. To build our tree, we start at the root node and ask a question that splits the data based on the feature such that the information gain is maximized. We continuously do this for each node until the decision tree can classify all the training data. Note that in practice this leads to overfitting, so the tree is usually pruned, i.e. a limit on the depth of the tree is set.</p>
<h2 id="information-gain">Information Gain</h2>
<p>We split each node on the feature that yields the most information gain. The formula for information gain in a binary decision tree is as follows:</p>
<p><span class="math display">\[IG(D_p, f) = I(D_p) - \frac{N_{left}}{N_p}I(D_{left}) - \frac{N_{right}}{N_p}I(D_{right})\]</span></p>
<p><span class="math inline">\(D_p\)</span> is the dataset of the parent node (the node which we are splitting), <span class="math inline">\(f\)</span> is the feature of the dataset which we are splitting on, <span class="math inline">\(N_p\)</span> is the total number of samples in the parent node, <span class="math inline">\(N_{left}\)</span> and <span class="math inline">\(N_{right}\)</span> are the number of samples in the datasets of the left child node and right child node respectively, and <span class="math inline">\(I\)</span> is the impurity measure. A node is pure if all samples in its dataset belong to the same class and is most impure when an equal number of samples belong to each class. Essentially, information gain calculates the difference between the impurity of the parent node and the impurity of the child nodes.</p>
<p>One commonly used measure of impurity is Gini impurity:</p>
<p><span class="math display">\[I_G(i) = 1 - \sum_{k=1}^{c} p(k|i)^2\]</span></p>
<p><span class="math inline">\(p(k|i)\)</span> is the proportion of samples of class <span class="math inline">\(k\)</span> to the total number of samples in the dataset of the <span class="math inline">\(i^{th}\)</span> node. The impurity is maximized when the classes of the node are perfectly mixed (for this example, consider a situation in which there are 2 classes, meaning c = 2): <span class="math display">\[1 - \sum_{k=1}^{c} 0.5^2 = 0.5\]</span></p>
<p>An alternative impurity measure is entropy, which is defined as:</p>
<p><span class="math display">\[-\sum_{k=1}^{c} p(k|i)log_2{p(k|i)}\]</span></p>
<p>Note that this function has a maximum of 1.0, not 0.5. In practice, Gini impurity and entropy yield similar results, so it is more useful to test different pruning cut-offs rather than to evaluate trees with different impurity criteria.</p>
<p>To decide on a split for a specific node, we will search for the feature and the threshold (e.g. “petal length &lt; 2.45 cm” for a flower classifier) that maximizes the information gain. One way to choose a good threshold is to select the best threshold from the 20%, 40%, 60%, and 80% quantiles of the feature set.</p>
<p>An snippet from a basic implementation of a decision tree might look like this:</p>
<h1 id="problems">Problems</h1>
<p>1. Write a full implementation of a decision tree from scratch. 2. Write a function that returns the entropy of a node in a decision tree.</p>
