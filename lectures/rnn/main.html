<h1 id="introduction">Introduction</h1>
<p>Recurrent Neural Networks (RNNs) are a type of neural network that specializes on sequential data; that is, data that is dependant on prior data. For instance, RNNs are often used for natural language processing, because a given word is more likely to appear depending on the prior words. In this vein, RNNs have been applied to tasks like machine translation, image caption generation, and word prediction. However, RNNs have also seen moderate successes in image generation and audio generation.</p>
<h1 id="basic-rnns">Basic RNNs</h1>
<p>In order to handle sequential data, a recurrent neural network has to be able to take in vectors of variable lengths. To do this, we design our network in the appropriate way. A RNN might look like the following:</p>
<div class="figure">
<img src="rnn.jpg" alt="An unrolled RNN." />
<p class="caption">An unrolled RNN.<span data-label="fig:rnn"></span></p>
</div>
<p>The figure above shows a RNN being <em>unrolled</em>, that is, being written out for the entire sequence <span class="math inline">\(x\)</span>. If, for example, we had a sentence composed of five words, we would have the network be unrolled into five layers, one for each word. Note that the number of layers in the network matches the length of the sequence, so we can handle variable-length vectors unlike standard feedfoward neural networks. Moreover, the hidden state <span class="math inline">\(s_t\)</span> depends on not only <span class="math inline">\(x_t\)</span> but also the state of the previous layer <span class="math inline">\(s_{t-1}\)</span>, allowing RNNs to process sequential data better than feedforward networks. The output of a layer, <span class="math inline">\(o_t\)</span>, is calculated from the layer’s hidden state and is thus likewise reliant on the previous hidden state. Specifically, <span class="math display">\[s_t = f(Ux_t + Ws_{t-1})\]</span> where U and W are weight matrices. The first hidden state <span class="math inline">\(s_0\)</span> is usually initialized to <span class="math inline">\(f(Ux_0)\)</span>. The function <span class="math inline">\(f\)</span> is typically the tanh function or the ReLU function. The output <span class="math inline">\(o_t\)</span> is calculated as such: <span class="math display">\[o_t = softmax(Vs_t)\]</span> where V is also a weight matrix. Note that U, V, and W are the same matrices for each layer as we are performing essentially the same operation at each time step <span class="math inline">\(t\)</span>, meaning we only need to learn a few parameters. Also note that although we have an output for each layer, we may not necessarily use every one. For example, in sentiment analysis, we would only want the final output, the sentiment of the entire sentence, not the sentiment after each individual word.</p>
<h2 id="backpropagation-through-time">Backpropagation Through Time</h2>
<p>The equations for the hidden state <span class="math inline">\(s_t\)</span> and predicted output <span class="math inline">\(o_t\)</span> are: <span class="math display">\[s_t=\tanh(U x_t+W s_{t-1})\]</span> <span class="math display">\[o_t= softmax(V s_t)\]</span></p>
<p>Our total loss (error) function is:</p>
<p><span class="math display">\[\label{loss}
    L(y,\hat{y})=\sum_{t} E_t\]</span></p>
<p>This is simply the sum of the losses at each time step. We can define the loss at each time step as: <span class="math display">\[\label{oneloss}
    E_t=-y_t\log o_t.\]</span> where <span class="math inline">\(y_t\)</span> is the ground truth at a particular time step <span class="math inline">\(t\)</span>. Note that the loss functions are dot products between the vectors <span class="math inline">\(y_t\)</span> and the resulting vector of the element-wise logarithm of <span class="math inline">\(o_t\)</span>. The weight matrices to be trained are <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span>, and <span class="math inline">\(W\)</span>. For <span class="math inline">\(V\)</span>, by the chain rule, we find: <span class="math display">\[\frac{\partial E_t}{\partial V} = \frac{\partial E_t}{\partial o_t}\frac{\partial o_t}{\partial (V s_t)}\frac{\partial (V s_t)}{\partial V}\]</span> From this, we can derive: <span class="math display">\[\frac{\partial E_t}{\partial V} = (o_t - y_t) \times s_t\]</span> Where <span class="math inline">\(\times\)</span> is the cross product. For <span class="math inline">\(W\)</span>, we can similarly apply the chain rule: <span class="math display">\[\frac{\partial E_t}{\partial W} = \frac{\partial E_t}{\partial o_t}\frac{\partial o_t}{\partial s_t}\frac{\partial s_t}{\partial W}\]</span> We can use the chain rule on the <span class="math inline">\(\frac{\partial s_t}{\partial W}\)</span> term again as <span class="math inline">\(s_t\)</span> depends on <span class="math inline">\(s_{t-1}\)</span> and we find: <span class="math display">\[\frac{\partial E_t}{\partial W} = \frac{\partial E_t}{\partial o_t}\frac{\partial o_t}{\partial s_t}\sum_{i=0}^t \frac{\partial s_t}{\partial s_i}\frac{\partial s_i}{\partial W}\]</span> From this, we can derive: <span class="math display">\[\frac{\partial E_t}{\partial W} = (o_t - y_t)V\sum_{i=0}^t \frac{\partial s_t}{\partial s_i}\frac{\partial s_i}{\partial W}\]</span> The partial derivative with respect to <span class="math inline">\(U\)</span> is essentially identical: <span class="math display">\[\frac{\partial E_t}{\partial U} = (o_t - y_t)V\sum_{i=0}^t \frac{\partial s_t}{\partial s_i}\frac{\partial s_i}{\partial U}\]</span></p>
<h2 id="problems-with-long-term-dependency">Problems with Long-term Dependency</h2>
<p>The <span class="math inline">\(\sum_{i=0}^t \frac{\partial s_t}{\partial s_i}\frac{\partial s_i}{\partial W}\)</span> term causes the vanishing gradient problem, which means that the network tends to forget words over time, which can prove troublesome. Let’s take a word predictor, for instance. Given part of a sentence, predict the next word. This structure would work quite well for a sentence like “I ate some delicious ice ____”; “ice” provides a significant cue that the next word might be “cream.” However, given a sentence like “I went to the ice cream store, but I forgot to bring my ____”, it will likely have trouble guessing “wallet.”</p>
<p>This is the problem of long-term dependencies; the network will “forget” words over time. We can remedy this issue with other types of RNNs such as LSTMs, which we will cover later on.</p>
<h1 id="types-of-rnns">Types of RNNs</h1>
<div class="figure">
<img src="LSTM.png" alt="An LSTM cell." />
<p class="caption">An LSTM cell.<span data-label="fig:lstm"></span></p>
</div>
<p>An Long Short Term Memory network (LSTM) attempts to alleviate this issue by having a “forget gate,” which allows the network to select parts of an input vector to “remember” (i.e. pass on to the next cell) or “forget.” It additionally has a mechanism to store and update the cell state. Let’s walk through the cell step-by-step.</p>
<h2 id="lstm-walk-through">LSTM walk-through</h2>
<div class="figure">
<img src="LSTM0.png" alt="Cell state." />
<p class="caption">Cell state.<span data-label="fig:lstm0"></span></p>
</div>
<p>First off, the cell state. This is the pathway in which information flows from cell-to-cell. It was designed so that this would have little interaction, so as to store long-term data with little change.</p>
<div class="figure">
<img src="LSTM1.png" alt="The forget gate." />
<p class="caption">The forget gate.<span data-label="fig:lstm1"></span></p>
</div>
<p>This is the forget gate. We use a sigmoid activation conditioned on the input and prior output, which gives us outputs between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Since we’re multiplying this value to the cell state, an output of <span class="math inline">\(0\)</span> would correspond to completely forgetting everything, <span class="math inline">\(1\)</span> remembering everything. This is done so as to update old information with more the more recent context.</p>
<div class="figure">
<img src="LSTM2.png" alt="Updating the cell state." />
<p class="caption">Updating the cell state.<span data-label="fig:lstm2"></span></p>
</div>
<div class="figure">
<img src="LSTM3.png" alt="Outputting the cell state." />
<p class="caption">Outputting the cell state.<span data-label="fig:lstm3"></span></p>
</div>
<p>Next, we add a vector to the cell state based on the input and previous output, so as to update the state. Note that we’re adding values between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>; <span class="math inline">\(\tanh\)</span> outputs <span class="math inline">\(-1\)</span> to <span class="math inline">\(1\)</span>, and the sigmoid outputs <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, which determines the magnitude of the update.</p>
<div class="figure">
<img src="LSTM4.png" alt="Outputting the final value." />
<p class="caption">Outputting the final value.<span data-label="fig:lstm4"></span></p>
</div>
<p>Lastly, we want to output <span class="math inline">\(h_i\)</span>. This is determined by the cell state, which is run through <span class="math inline">\(\tanh\)</span> to bound the values between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. The sigmoid, whose output is conditioned on the previous cell output and current input, allows the cell to select which parts of the cell state to output (the sigmoid will output <span class="math inline">\(0\)</span> for the parts it does not want to output, and vice-versa).</p>
<p>Then, the process repeats, with the next input.</p>
<h2 id="grus">GRUs</h2>
<div class="figure">
<img src="GRU.png" alt="A GRU cell." />
<p class="caption">A GRU cell.<span data-label="fig:gru"></span></p>
</div>
<p>A Gated Recurrent Unit (GRU) is an alternative to an LSTM that is computationally simpler. It opts to combine the update and forget mechanisms into an “update” mechanism, and also combines the hidden state and the cell state. GRUs are becoming increasingly popular as the perform as well as LSTMs on some tasks at a significantly lower computational cost.</p>
<h1 id="conclusion">Conclusion</h1>
<p>There are many other types of RNNs; LSTMs and GRUs are two of the most popular. Additionally, there have been advancements on RNNs themselves; “attention” mechanisms are popular in tasks such as machine translation and caption generation.</p>
<p>For NLP tasks, we recommend investigating word vectors (Word2Vec, skip-thought vectors, etc.) to encode words to vectors. Also, using RNNs as the basis for an encoder/decoder model is widely used to translate sequences (e.g. machine translation). We encourage you to further investigate these areas!</p>
