\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Intro to Keras}
\author{Justin Zhang}
\date{July 2017}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{listings}

\begin{document}

\maketitle

\section{Introduction}
    Keras is a high-level Python machine learning API, which allows you to easily run neural networks. Keras is simply a specification; it provides a set of methods that you can use, and it will use a backend (TensorFlow, Theano, or CNTK, as chosen by the user) to actually run your code. Like many machine learning frameworks, Keras is a so-called \textit{define-and-run} framework. This means that it will define and optimize your neural network in a compilation step before training starts.
\section{First Steps}
    First, install Keras: \verb|pip install keras|
    
    We'll go over a fully-connected neural network designed for the MNIST (classifying handwritten digits) dataset.
    \lstinputlisting[language=Python, firstline=1, lastline=8]{mnist_mlp.py}
    First, we handle the imports. \verb|keras.datasets| includes many popular machine learning datasets, including CIFAR and IMDB. \verb|keras.layers| includes a variety of neural network layer types, including \verb|Dense|, \verb|Conv2D|, and \verb|LSTM|. \verb|Dense| simply refers to a fully-connected layer, and \verb|Dropout| is a layer commonly used to address the problem of overfitting. \verb|keras.optimizers| includes most widely used optimizers. Here, we opt to use RMSprop, an alternative to the classic stochastic gradient decent (SGD) algorithm. Regarding \verb|keras.models|, \verb|Sequential| is most widely used for simple networks; there is also a functional \verb|Model| class you can use for more complex networks.
    \lstinputlisting[language=Python, firstline=10, lastline=12]{mnist_mlp.py}
    Here, we define our constants...
    \lstinputlisting[language=Python, firstline=14, lastline=15]{mnist_mlp.py}
    ...and the dataset.
    \lstinputlisting[language=Python, firstline=17, lastline=22]{mnist_mlp.py}
    The training set will contain $60000$ $28$ by $28$ images; we resize this to $60000$ by $784$ since our MLP will take as input a vector of length $784$. Similarly, the test set, which consists of $10000$ images is resized to $10000$ by $784$. We next convert these tensors to floats, and normalize the values to $[0,1]$. 
    \lstinputlisting[language=Python, firstline=24, lastline=26]{mnist_mlp.py}
    This code converts the ground-truth labels from a class vector (each digit is labeled with an integer $0$ through $9$) to a one-hot encoding (each digit is labeled with a length $9$ vector which consists of zeros except for the index represented by the digit, which equals one). This is so that the ground-truth labels will be compatible with our categorical cross-entropy loss function.
    \lstinputlisting[language=Python, firstline=28, lastline=33]{mnist_mlp.py}
    Here, the neural network is defined. The inputs are $784 \rightarrow 512 \rightarrow 512 \rightarrow 10$, with the ReLU activation function (alternative to the classical sigmoid). A softmax is applied at the end to convert our ten outputs to probabilities, each output denoting a digit. Dropout is applied in between layers to avoid overfitting.
    \lstinputlisting[language=Python, firstline=35, lastline=37]{mnist_mlp.py}
    Here, the model is compiled. Our loss function is categorical cross-entropy, which is used for categorical data with more than two classes. The \verb|metrics| keyword argument is a list of metrics we want to keep track of in throughout training, validation, and testing. We can provide our own functions, or provide strings denoting built-in metrics (e.g. accuracy). 
    \lstinputlisting[language=Python, firstline=39, lastline=43]{mnist_mlp.py}
    Here, the model is trained. The \verb|verbose| keyword argument gives us a nice progress bar as the training process progresses. Returned is a history of the metrics throughout training.
    \lstinputlisting[language=Python, firstline=44, lastline=44]{mnist_mlp.py}
    Finally, the model is evaluated on the test set. The metrics, as defined in the compile function, are returned.
\section{More Stuff}
    Though this was a fairly simple example, it covers mostly what you will use in Keras. More advanced layers, optimizers, or losses can be found at Keras's homepage [https://keras.io/]; Keras has great documentation! Things that may interest you are Keras's functional API, which allows more flexible networks with multiple inputs/outputs, as well as the backend API, which allows for defining custom layers. 
    Note that Keras and TensorFlow are interoperable; you can write TensorFlow code that operates on Keras tensors, for defining operations at an even lower level than that specified in the backend API (given that you use the TensorFlow backend for Keras).
    
\section{Conclusion}
    Keras is a great framework for rapid prototyping; it provides a high-level API with potential for lower level operations, when necessary. Note that, however, when working with large or customized models over long-term projects, you may want to look into other frameworks such as PyTorch, which doesn't have a compile step and is much more low-level.
\section{Practice}
    Do Kaggle competitions!


\end{document}
