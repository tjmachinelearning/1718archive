<h1 id="introduction">Introduction</h1>
<p>We have covered Perceptrons, the fundamental unit of the Neural Network (See “Neural Networks: Introduction and Overview” for the previous lecture). Additionally, Multi-Layer Perceptrons, or Neural Networks, were introduced as a solution for approximating non-linearly separable data. This lecture continues exploring Neural Networks, but introduces vectorization for more efficient notation and computation. We also cover how neural networks learn, which is significantly more complex than the perceptron learning algorithm.</p>
<h1 id="the-neuron">The Neuron</h1>
<p>A single node of a neural network (a neuron) differs from a perceptron in one way: the activation function. Consider this diagram of a neuron:</p>
<p><img src="neuron" alt="image" /></p>
<p>The symbol <span class="math inline">\(\sigma\)</span> represents the Sigmoid activation function <span class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span>.</p>
<p>; ; ;</p>
<p>Notice how as the coefficent of <span class="math inline">\(x\)</span> approaches infinity, <span class="math inline">\(\sigma(x)\)</span> approaches the step function from before. We use <span class="math inline">\(\sigma(x)\)</span> is because it is differenciable, which is necessary for networks to learn. Other activation functions include <span class="math inline">\(\tanh(x)\)</span> and ReLU, but we will use Sigmoid for our examples.</p>
<p>The rest of a neuron is identical to a perceptron: multipy each input by its weight, add them up and the bias and compute the activation function of the sum.</p>
<h1 id="forward-propagation">Forward Propagation</h1>
<h2 id="non-vectorized-forward-propagation">Non-Vectorized Forward Propagation</h2>
<p>Forward Propagation is a fancy term for computing the output of a neural network. We must compute all the values of the neurons in the second layer before we begin the third, but we can compute the individual neurons in any given layer in any order. Consider the following network:</p>
<p><img src="nn1" alt="image" /></p>
<p>We denote the value of node i as <span class="math inline">\(n_i\)</span>, and the bias of node i as <span class="math inline">\(b_i\)</span>. Computing the network using these variables, we get: <span class="math display">\[n_3 = \sigma(w_{13}n_1 + w_{23}n_2 + b_3)\]</span> <span class="math display">\[n_4 = \sigma(w_{14}n_1 + w_{24}n_2 + b_4)\]</span> <span class="math display">\[n_5 = \sigma(w_{15}n_1 + w_{25}n_2 + b_5)\]</span> <span class="math display">\[n_6 = \sigma(w_{36}n_3 + w_{46}n_4 + w_{56}n_5 + b_6)\]</span> Continuing this example of forward propagation, let’s assign some numbers and compute the output of this network. Let <span class="math inline">\(n_1 = 0.2\)</span> and <span class="math inline">\(n_2 = 0.3\)</span>. Let <span class="math inline">\(w_{13} = 4, w_{14} = 5, w_{15} = 6, w_{23} = 5, w_{24} = 6, w_{25} = 7, w_{36}=9, w_{46} = 10\)</span> and <span class="math inline">\(w_{56} = 11\)</span>, just so they are easy to remember. Let all the biases <span class="math inline">\(b_{3..6} = 1\)</span> (input nodes do not have biases, the “input nodes” are simply values given to the network). In practice, weights and biases of a network are initialized randomly between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. Given these numbers, we compute: <span class="math display">\[n_3 = \sigma(4*0.2 + 5*0.3 + 1) = \sigma(3,3) = 0.964\]</span> <span class="math display">\[n_4 = \sigma(5*0.2 + 6*0.3 + 1) = \sigma(3.8) = 0.978\]</span> <span class="math display">\[n_5 = \sigma(6*0.2 + 7*0.3 + 1) = \sigma(4.3) = 0.987\]</span> <span class="math display">\[n_6 = \sigma(9*0.964 + 10*0.978 + 11*0.987 + 1) = \sigma(30.313) = 1\]</span></p>
<p>This example actually illustrates one of the weak points of the Sigmoid function: it quickly approaches 1 for large numbers. The reason for using the Sigmoid function will be shown in the section on backpropagation.</p>
<h2 id="vectorized-forward-propagation">Vectorized Forward Propagation</h2>
<p>Look again at these nodes of the network: <span class="math display">\[n_3 = \sigma(w_{13}n_1 + w_{23}n_2 + b_3)\]</span> <span class="math display">\[n_4 = \sigma(w_{14}n_1 + w_{24}n_2 + b_4)\]</span> <span class="math display">\[n_5 = \sigma(w_{15}n_1 + w_{25}n_2 + b_5)\]</span></p>
<p>We can rewrite this as <span class="math display">\[\begin{bmatrix}
    n_{3}\\
    n_{4}\\
    n_{5}\\
\end{bmatrix}
=
\sigma
\Bigg(
\begin{bmatrix}
    w_{13}       &amp; w_{23}\\
    w_{14}       &amp; w_{24}\\
    w_{15}       &amp; w_{25}\\
\end{bmatrix}
\begin{bmatrix}
    n_{1}\\
    n_{2}\\
\end{bmatrix}
+
\begin{bmatrix}
    b_{3}\\
    b_{4}\\
    b_{5}\\
\end{bmatrix}
\Bigg)\]</span> Notice how the nodes in each layer of the network are in their own column vector, in the order they appear. Let’s relabel this network by layers:</p>
<p>NETWORK GOES HERE</p>
<p>Here, <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_2\)</span> represent the input and output layers, and <span class="math inline">\(x_1\)</span> is the middle layer (called a hidden layer). Mathematically speaking, these are represented as column vectors of dimension <span class="math inline">\(n\times1\)</span>, where <span class="math inline">\(n\)</span> is the number of nodes in the layer. Thinking back to the non-vectorized network in section 3.1, <span class="math display">\[x_0 =
\begin{bmatrix}
    n_{1}\\
    n_{2}\\
\end{bmatrix}
\qquad
x_1 =
\begin{bmatrix}
    n_{3}\\
    n_{4}\\
    n_{5}\\
\end{bmatrix}
\qquad
x_2 =
\begin{bmatrix}
    n_{6}\\
    n_{7}\\
\end{bmatrix}\]</span> <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are the weight matrices. Thinking back to the non-vectorized network in section 3.1, <span class="math inline">\(w_1\)</span> corresponds to <span class="math display">\[\begin{bmatrix}
    w_{13}       &amp; w_{23}\\
    w_{14}       &amp; w_{24}\\
    w_{15}       &amp; w_{25}\\
\end{bmatrix}\]</span> and <span class="math inline">\(w_2\)</span> refers to <span class="math display">\[\begin{bmatrix}
    w_{36}\\
    w_{46}\\
    w_{56}\\
\end{bmatrix}\]</span> Each layer (except the input) also has a bias vector, which has the same dimension as the layer itself (each node has a bias). Again thinking back to the non-vectorized network in section 3.1, we define <span class="math inline">\(b_1\)</span> to be <span class="math display">\[\begin{bmatrix}
    b_{3}\\
    b_{4}\\
    b_{5}\\
\end{bmatrix}\]</span> and <span class="math inline">\(b_2\)</span> to be <span class="math display">\[\begin{bmatrix}
    b_{6}\\
    b_{7}\\
\end{bmatrix}\]</span> We can now re-write the forward propagation formula in a far more compact form. In any <span class="math inline">\(n\)</span> layer network, for a given layer <span class="math inline">\(x_{i+1}\)</span> (assuming <span class="math inline">\(0\leq i&lt;n-1\)</span>): <span class="math display">\[x_{i+1} = \sigma(w_ix_i + b_{i+1})\]</span></p>
<h1 id="backpropagation">Backpropagation</h1>
<p>Backpropagation is how neural networks learn. It is essential to not only understand the theory behind backpropagation, but also the mathematics behind it. This is one of the few mathetmatically rigorous sections of our material. (Of course, for anyone who has taken Multivariable calculus, the material should be relatively straightforward. Nevertheless, high school students not entirely comfortable with the math will no doubt have some trouble.)</p>
<h2 id="learning">Learning</h2>
<p>A neural network learns when it is given training data and labels. The data (inputs) can be in the form of text, images, numbers, etc. The label is the ground truth, the correct answer for the given input. Given enough data-label pairs, a network can learn to generalize the relationship between the data and label. After training, it is tested or validated on a set of data it has never seen before (i.e. data not part of the training set). This validation accuracy shows just how well a network has learned to generalize through training. Backpropagation is the method of updating the weights and biases of the network to minimize the error when training.</p>
<h2 id="error">Error</h2>
<p>Consider the following network:</p>
<p>NETWORK GOES HERE</p>
<p>For the input <span class="math inline">\(x_0\)</span>, let <span class="math inline">\(y\)</span> represent the target vector, or the ground truth. We define the error as <span class="math display">\[E = \frac{1}{2}||x_2-y||^2\]</span></p>
<p>Essentially, this is the magnitude of the difference between the target and the network’s output. In order for a network to become more accurate, we want to minimize this error.</p>
<p>Let’s think of <span class="math inline">\(E\)</span> as a function. Only <span class="math inline">\(x_2\)</span> can vary, and we can only control this by changing the weight matrices (and the bias). Thus, for a neuron with <span class="math inline">\(n\)</span> weights and a bias, the error can be graphed as an <span class="math inline">\(n+2\)</span> dimensional function (<span class="math inline">\(y = f(x)\)</span> has 1 input, so it is graphed in two dimensions). For this network, each of the weights (<span class="math inline">\(3*4 + 4*2 = 20\)</span>) and the biases (<span class="math inline">\(6\)</span>) determines the error, so the error has many, many dimensions. If we get to the minimum of this function, we have minimized the error and trained the network.</p>
<h2 id="gradient-descent">Gradient Descent</h2>
<p>We can’t visualize that many dimensions (at least, I can’t), so lets pretend we are working with a three dimensional function. How to we get to the minimum? We use gradient descent, of course!</p>
<p><img src="gd" alt="image" /></p>
<p>A multi-dimensional function. Look at that minimum!</p>
<p>Gradient descent is simple: Starting at some point, we move in the direction of steepest decline for a certain length. Then, at our new point, we again compute the direction of steepest decline, and move in that direction for a certain length. We repeat this process over and over until every single direction is an incline, at which point we are at the minimum.</p>
<p>This has three issues. First, how do we know how long our steps are? Take a step too long, and we could overshoot the minimum. Take a step too short and it will take us many steps to reach the minimum. The step length is actually just a constant set by the programmer, and normally ranges from <span class="math inline">\(0.1\)</span> to <span class="math inline">\(0.0001\)</span>. Adjusting the constant to get the best result is an important practical topic for getting the best result, and we will discuss it in Part 3 of the lecture. For now, just know its a constant.</p>
<p>Secondly, doesn’t gradient descent just get us to a minimum? What if there are multiple minima, and we just happen to land in a local minimum, like the many in the function below?</p>
<p><img src="minima" alt="image" /></p>
<p>Getting out of local minima to reach the global minimum is another important machine learning topic. Different optimizers can help the network pop out of local minima using momentum, but this topic is complex and modern, so it is covered in depth in Part 3 of this lecture. For the purposes of explaining gradient descent, we’ll just pretend we’re working with an error function with one minimum.</p>
<p>The third and final issue is: how do we know which direction is the steepest? We can’t just sample each direction, as there are infinite possibilities. Instead, we mathematically compute the best direction. Let’s consider a simple two-dimensional parabola:</p>
<p><img src="sgd_optimal" alt="image" /></p>
<p>From elementary calculus, we know that:</p>
<p><span class="math display">\[f(x) = x^2\]</span> <span class="math display">\[f&#39;(x) = 2x\]</span></p>
<p>The derivative gives us the instantaneous rate of change for any <span class="math inline">\(x\)</span>. If we have a function in terms of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, we can take the derivative of <span class="math inline">\(f(x, y)\)</span> with respect to <span class="math inline">\(x\)</span> to find the rate of change in the x direction, and the derivative with respect to <span class="math inline">\(y\)</span> to find the rate of change in the y direction. These are called partial derivatives. We treat the other variables like we would any other constant.</p>
<p>Let’s do an example. Given <span class="math inline">\(f(x, y) = 2x^2 + 3xy + y^3\)</span>, the partial derivatives are: <span class="math display">\[\frac{\partial f}{\partial x} = 4x + 3y\]</span> <span class="math display">\[\frac{\partial f}{\partial x} = 3x + 3y^2\]</span></p>
<p>The gradient of <span class="math inline">\(f(x, y)\)</span>, or <span class="math inline">\(\nabla f(x, y)\)</span> is just the vector: <span class="math display">\[\big(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\big)\]</span></p>
<p>For our example, the gradient is: <span class="math display">\[(4x + 3y,3x + 3y^2)\]</span> This is the direction of steepest ascent. How do we know that? First, lets consider the directional derivative. <span class="math inline">\(\nabla_u f(x_0, y_0)\)</span> is the rate of change of <span class="math inline">\(f(x, y)\)</span> at the point <span class="math inline">\((x_0, y_0)\)</span> in the direction <span class="math inline">\(\vec u\)</span>. It is also defined in terms of the gradient as: <span class="math display">\[\nabla_u f(x_0, y_0) = \nabla f(x, y) \cdot \vec u\]</span></p>
<p>We know from our standard dot product rule: <span class="math display">\[\vec a \cdot \vec b = ||\vec a ||||\vec b||\cos(\theta)\]</span></p>
<p>And <span class="math inline">\(\cos(\theta)\)</span> is maximized at <span class="math inline">\(\theta = 0\)</span>. Thus, when two vectors are in the same direction, their dot product is maximized. From this information, the maximum of the directional derivative must be when <span class="math inline">\(\nabla f(x, y)\)</span> and <span class="math inline">\(\vec u\)</span> are in the same direction. This means that the direction of the steepest ascent (maximum rate of change) is the direction of the gradient.</p>
<p>Great! Now our third issue has been solved. In order to find the minimum of a multi-dimensional function, we just need to compute the gradient, move in that direction for a certain length, and repeat until the gradient is 0. The only problem is.... how do we compute the gradient? Our function is</p>
<p><span class="math display">\[E(W, b) = \frac{1}{2}||o-t||^2\]</span></p>
<p>Where <span class="math inline">\(o\)</span> is the network output at <span class="math inline">\(t\)</span> is the target. Since the error is in terms of the weights and biases, that means that we need to compute: <span class="math display">\[\big(\frac{\partial E}{\partial W_1}, \frac{\partial E}{\partial W_2}, ..., \frac{\partial E}{\partial b_n}\big)\]</span></p>
<p>This is why backpropagation is a fundamental concept in machine learning. It allows us to compute this gradient in a computationally efficient manner.</p>
<h2 id="non-vectorized-backpropagation">Non-Vectorized Backpropagation</h2>
<p>Don’t do it. Seriously.</p>
<h2 id="vectorized-backpropagation">Vectorized Backpropagation</h2>
<p>Consider the network from Section 4.2 again.</p>
<p>NETWORK GOES HERE</p>
<p>Ignoring biases (which we will see follow a relatively simple rule), we know from forward propagation that: <span class="math display">\[x_1 = \sigma(W_1x_0)\]</span> <span class="math display">\[x_2 = \sigma(W_2x_1)\]</span></p>
<p>And the error is, assuming some <span class="math inline">\(2 \times 1\)</span> target vector <span class="math inline">\(y\)</span>: <span class="math display">\[E = \frac{1}{2}||x_2-y||^2\]</span></p>
<p>Let’s first take the partial derivative of <span class="math inline">\(E\)</span> with respect to <span class="math inline">\(W_2\)</span>. This is just like taking a normal derivative (using the chain rule). <span class="math display">\[\frac{\partial E}{\partial W_2} = (x_2 - y) \frac{\partial (\sigma(W_2x_1))}{\partial W_2}\]</span> <span class="math display">\[\frac{\partial E}{\partial W_2} = [(x_2 - y) \odot \sigma&#39;(W_2x_1)] \frac{\partial W_2x_1}{\partial W_2}\]</span></p>
<p>Here, <span class="math inline">\(\odot\)</span> is the Hadamard product, or element wise multiplication (Remember, these are all vectors). For the sake of simplification, lets define <span class="math display">\[\delta_2 = (x_2 - y) \odot \sigma&#39;(W_2x_1)\]</span></p>
<p>Then, we can rewrite the partial as <span class="math display">\[\frac{\partial E}{\partial W_2} = \delta_2 \frac{\partial W_2x_1}{\partial W_2} = \delta_2 x_1^T\]</span></p>
<p>Note that <span class="math inline">\(x_1^T\)</span> means that the <span class="math inline">\(x_1\)</span> vector has been transposed (i.e. it is a row vector). This is essential for the dimensions to work out, which we can check now.</p>
<p>Since the whole point is to update the weights by some factor every time we backpropagate in the direction of fastest descent to minimize the error, we want to subtract the partial matrix (since it is in the direction of fastest ascent):</p>
<p><span class="math display">\[W_i =  W_i - \alpha \frac{\partial E}{\partial W_i}\]</span> where alpha is the learning rate. This requires <span class="math inline">\(\frac{\partial E}{\partial W_i}\)</span> to be the same dimensions as <span class="math inline">\(W_i\)</span>. Using <span class="math inline">\(W_2\)</span> as an example, we know that <span class="math display">\[x_2 = \sigma(W_2x_1)\]</span> where <span class="math inline">\(x_2\)</span> is a <span class="math inline">\(2 \times 1\)</span> vector, <span class="math inline">\(x_1\)</span> is a <span class="math inline">\(4 \times 1\)</span> vector, so <span class="math inline">\(W_2\)</span> is a <span class="math inline">\(2 \times 4\)</span> matrix. Thus, both <span class="math inline">\(\frac{\partial E}{\partial W_i}\)</span> and <span class="math inline">\(\delta_2 x_1^T\)</span> are also <span class="math inline">\(2 \times 4\)</span> matrices. Since <span class="math inline">\(\delta_2 = (y - \sigma(W_2x_1)) \odot \sigma&#39;(W_2x_1)\)</span>, and we know <span class="math inline">\(y\)</span> is a <span class="math inline">\(2 \times 1\)</span> matrix, <span class="math inline">\(\delta_2\)</span> has dimensions <span class="math inline">\(2 \times 1\)</span>. If <span class="math inline">\(\delta_2\)</span> is <span class="math inline">\(2 \times 1\)</span>, then it must be multiplied by a <span class="math inline">\(1 \times 4\)</span> vector to create a <span class="math inline">\(2 \times 4\)</span> matrix. Since <span class="math inline">\(x_1\)</span> is <span class="math inline">\(4 \times 1\)</span>, it must be transposed to become <span class="math inline">\(1 \times 4\)</span>.</p>
<p>Let’s continue to the next weight matrix. <span class="math display">\[\frac{\partial E}{\partial W_1} = (x_2 - y) \frac{\partial (\sigma(W_2x_1))}{\partial W_1}\]</span> <span class="math display">\[\frac{\partial E}{\partial W_1} = [(x_2-y) \odot \sigma&#39;(W_2x_1)] \frac{\partial W_2x_1}{\partial W_1}\]</span> <span class="math display">\[\frac{\partial E}{\partial W_1} = \delta_2 \frac{\partial W_2x_1}{\partial W_1} = W_2^T\delta_2 \frac{\partial x_1}{\partial W_1}\]</span> Substituting in for <span class="math inline">\(x_1\)</span>, we get: <span class="math display">\[\frac{\partial E}{\partial W_1} = W_2^T\delta_2 \frac{\partial (\sigma(W_1x_0))}{\partial W_1}\]</span> <span class="math display">\[\frac{\partial E}{\partial W_1} = [W_2^T\delta_2 \odot \sigma&#39;(W_1x_0)]\frac{\partial W_1x_0}{\partial W_1}\]</span></p>
<p>Again, we simplify this: <span class="math display">\[\delta_1 = W_2^T\delta_2 \odot \sigma&#39;(W_1x_0)\]</span> and we finish with <span class="math display">\[\frac{\partial E}{\partial W_1} = \delta_1 \frac{\partial W_1x_0}{\partial W_1}\]</span> <span class="math display">\[\frac{\partial E}{\partial W_1} = \delta_1 x_0^T\]</span></p>
<p>We can generalize this for any layer. The only difference is the delta for the last layer: <span class="math display">\[\delta_L = (x_L - y) \odot \sigma&#39;(W_{L}x_{L-1})\]</span></p>
<p>The delta for every other layer is: <span class="math display">\[\delta_i = W_{i+1}^T\delta_{i+1} \odot \sigma&#39;(W_ix_{i-1})\]</span></p>
<p>And the gradient for every weight matrix are calculated and the weight matrices are updated as follows: <span class="math display">\[\frac{\partial E}{\partial W_i} = \delta_i x_{i-1}^T\]</span></p>
<p><span class="math display">\[W_i =  W_i - \alpha \frac{\partial E}{\partial W_i}\]</span></p>
<p>For biases, the rule is simpler:</p>
<p><span class="math display">\[b_i =  b_i - \alpha \delta_i\]</span></p>
<p>That is the essence of backpropagation. Note that these formulas work for any activation function. The reason sigmoid is used to teach is because its derivative is fairly straightforward: <span class="math display">\[\sigma&#39;(x) = \sigma(x)(1-\sigma(x))\]</span></p>
<h1 id="problems">Problems</h1>
<ol>
<li><p>Given the following network:</p>
<p>NETWORK GOES HERE</p>
<p>the weight matrices, bias vectors and input are as follows: <span class="math display">\[W_1 =
    \begin{bmatrix}
        2 &amp;&amp; 3 &amp;&amp; 4 \\
        2 &amp;&amp; 1 &amp;&amp; 2 \\
        3 &amp;&amp; 5 &amp;&amp; 1 \\
        2 &amp;&amp; 3 &amp;&amp; 4 \\
    \end{bmatrix}
    \qquad
    W_2 =
    \begin{bmatrix}
        3 &amp;&amp; 1 &amp;&amp; 1 &amp;&amp; 1\\
        1 &amp;&amp; 4 &amp;&amp; 2 &amp;&amp; 2\\
    \end{bmatrix}\]</span> <span class="math display">\[x_0 =
    \begin{bmatrix}
        2 \\
        1 \\
        3 \\
    \end{bmatrix}
    \qquad
    b_1 =
    \begin{bmatrix}
        4 \\
        1 \\
        1 \\
        2 \\
    \end{bmatrix}
    \qquad
    b_2 =
    \begin{bmatrix}
        2 \\
        3 \\
    \end{bmatrix}\]</span> Instead of using the Sigmoid activation function, use a linear function <span class="math inline">\(y=x\)</span>, which always has a derivative of 1. Compute the output of one forward pass, then compute a backward pass using the following target and learning rate: <span class="math display">\[t =
    \begin{bmatrix}
        4 \\
        5 \\
    \end{bmatrix}
    \qquad
    \alpha = 0.1\]</span></p></li>
<li><p>Write out the forward propagation algorithm in Python. Use the Numpy library for matrices.</p></li>
<li><p>Write out the backpropagation algorithm in Python. Use the Numpy library for matrices.</p></li>
<li><p>Write an entire Neural Network in Python, using the Numpy library.</p></li>
</ol>
