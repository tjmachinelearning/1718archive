<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Object Detection | TJHSST Machine Learning Club</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../css/demo.css" />
    <link rel="stylesheet" type="text/css" href="../../css/component.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../manifest.json">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#5bbad5">
    <meta name="theme-color" content="#ffffff">
    <meta name=”description” content="A comprehensive overview of the latest object detection networks: R-CNN, Fast R-CNN, Faster R-CNN, and Single Shot Detector (SSD).">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-105333430-1', 'auto');
      ga('send', 'pageview');

    </script>
  </head>
  <body>
    <div class="container">
        <h2 style="text-align:center"><a href="../../index.html"><img src="../../img/logo.svg" width="200px"></img></a></h2>
         <section class="section section--menu" id="Alonso">
             <span class="link-copy"></span>
             <nav class="menu menu--alonso">
                 <ul class="menu__list">
                     <li class="menu__item"><a href="../../index.html" class="menu__link">Home</a></li>
                     <li class="menu__item menu__item--current"><a href="../../schedule.html" class="menu__link">Lectures</a></li>
                     <li class="menu__item "><a href="../../rankings.html" class="menu__link">Rankings</a></li>
                     <li class="menu__item"><a href="../../resources.html" class="menu__link">Resources</a></li>
                     <li class="menu__item"><a href="../../projects.html" class="menu__link">Projects</a></li>
                     <li class="menu__item"><a href="../../submit.html" class="menu__link">Updates</a></li>
                     <li class="menu__line"></li>
                 </ul>
             </nav>
         </section>
    </div>
    <section class="main-content">
        <div class="lecture">
            <h1 style="text-align:center; color:#000">Object Detection</h1>
            <h3 style="text-align:center; color:#000">Nikhil Sardana</h2>
            <h3 style="text-align:center; color:#000">December 2017</h2>

                <h1 id="introduction">Introduction</h1>
                <p>Convolutional Neural Networks (CNNs) allow us to classify images. However, it is reasonable to ask if we use Convolutional networks to detect and classify objects in images. Object detection is applicable to a wide variety of fields, including autonomous driving, video surveillance, facial detection, and manufacturing. For example, in a self-driving car, cameras must be able to detect street signs, pedestrians, lane lines, guardrails, buildings, and more, quickly and accurately to prevent crashes.</p>
                <p>This lecture discusses four object detection networks: R-CNN, Fast R-CNN, Faster R-CNN, and Single Shot Detectors (SSD). Fast R-CNN and Faster R-CNN modify the R-CNN architecture to improve speed and performance. The Single Shot Detector is an entirely new network design. This lecture is intended for readers with understanding of traditional Convolutional networks, but no prior knowledge of object detection networks. Understanding of Fully Convolutional Networks is recommended prior to reading the “Faster R-CNN&quot; section.</p>
                <h1 id="r-cnn">R-CNN</h1>
                <p>The Region-based Convolutional Neural Network (R-CNN) is relatively simple. First, we propose some regions of an image which contain objects. We feed these regions into a CNN, and then we use an SVM to classify the outputs for each class.</p>
                <p style="text-align:center"><img src="rcnn.png" alt="image" /></p>
                <p>If we wish to have a CNN classify objects in images, we need to feed in a region of the image to the CNN. Of course, the question becomes: How do we know which regions to feed into a network? We cannot possibly try every single possible region of an image; there are too many combinations. We must have a way to propose regions which are likely to contain objects.</p>
                <div class="figure" style="text-align:center">
                <img src="search.PNG" alt="Too many regions!" style="width:66.0%"/>
                <p class="caption">Too many regions!</p>
                </div>
                <div class="figure" style="text-align:center">
                <img src="region.PNG" alt="Too many regions!" style="width:50.0%" />
                <p class="caption">Only some are useful.</p>
                </div>
                <p>R-CNN is agnostic to the region proposal algorithm. The original R-CNN uses the Selective Search algorithm, which we will describe below. Since selective search, various region proposal methods have been developed.</p>
                <p style="text-align:center"><img src="table.PNG" alt="image" /></p>
                <h2 id="selective-search">Selective Search</h2>
                <p>Selective Search is based upon the principle of segmentation to propose regions. If we can segment an image into objects, we can simply surround the segmented areas with rectangles, and use the rectangles as our input regions into the CNN.</p>
                <p style="text-align:center"><img src="segment.PNG" alt="image" /></p>
                <p>Wouldn’t we need another CNN to perform the segmentation? How could this possibly be efficient? In fact, we will talk about using CNNs for segmentation later, but Selective Search uses an efficient segementation technique to generate region proposals. Selective Search is efficient simply because we only care about recall:</p>
                <p><span class="math display">\[Recall = \frac{TP}{TP + FN}\]</span></p>
                <p>In other words, we don’t care about the number of False Positives (<span class="math inline">\(FP\)</span>) regions, or regions without any objects, as long as each of the objects in the image is in at least one proposed region. Our CNN can take care of the false positives, but if all of our proposed regions completely miss an object, we never get the chance to classify it at all.</p>
                <h3 id="segmentation">Segmentation</h3>
                <p>The actual segmentation algorithm used by Selective Search is not novel at all. Rather, Selective Search uses the algorithm proposed in “Efficient Graph-Based Image Segmentation&quot; by Felzenszwalb et al. (2004). The algorithm is decidedly non-trivial, and I will not attempt to explain it here. However, two things are important for us to know.</p>
                <p>First, the segmentation algorithm is highly efficient, running in <span class="math inline">\(O(n\log n)\)</span> time to the number of pixels. Second, the algorithm tends to over-segment. Notice how the letters on the lower baseball player’s jersey are segmented from the rest of the jersey. The jersey is segmented from the head of the player.</p>
                <p style="text-align:center"><img src="graphbasedsegmentation.PNG" alt="image" /></p>
                <p>None of these segmented regions fully contains an object (a baseball player), and thus, none of them are suitable for drawing a box around and using as an input for our CNN.</p>
                <p style="text-align:center"><img src="selective1.PNG" alt="image" /></p>
                <p>We wish to somehow separate objects from one another and the background, while maintaining parts of an object together which may look nothing alike. For example, in the car below, the wheels are part of the car, and yet have a different size, shape, and color than the body of the car. The chameleon, on the other hand, is the same color as the leaves around it; it is distinguishable only by texture.</p>
                <div class="figure" style="text-align:center">
                <img src="car.jpg" alt="Texture distinguishes the chameleon." style="width:80.0%" />
                <p class="caption">Texture distinguishes the chameleon.</p>
                </div>
                <div class="figure" style="text-align:center">
                <img src="texture.jpg" alt="Texture distinguishes the chameleon." style="width:80.0%" />
                <p class="caption">Texture distinguishes the chameleon.</p>
                </div>
                <p>This is where Selective Search comes in. Selective Search proposes a novel method of combining alike segmented regions, and recursively merging them to encompass entire objects while simultaneously reducing the number of proposed regions.</p>
                <h3 id="recursive-combination">Recursive Combination</h3>
                <p>The figure below shows Selective Search after progressively more iterations. Blue boxes are false positive candidate objects, green boxes are true positive candidate objects. By recursively combining segmented regions, Selective Search drastically cuts down on false positives while increasing true positives.</p>
                <p style="text-align:center"><img src="selective2.PNG" alt="image" /></p>
                <p>Selective Search is greedy. From a set of regions, we choose the two that are most similar, and combine them into a single region. We repeat this until a single region remains (of course, we never actually reach a single region).</p>
                <p>The question then becomes: How do we decide which regions to combine? In other words, how do we determine <em>similarity</em> between regions?</p>
                <h3 id="similarity-metrics">Similarity Metrics</h3>
                <p>There are four similarity metrics: color, texture, size, and shape compatibility. We could simply use RGB color, but changes in shadows and light intensity affect RGB values. If an object is unevenly lit, we want color values to remain the same throughout. So, we can use HSV (hue, saturation, value) color, which is invariant to lighting changes. Saturation is insensitive to shadows, and value is insensitive to brightness change. An alternative is <em>Lab</em> color (below HSV diagram), which is designed to approximate human vision, The <span class="math inline">\(L\)</span> component matches the human perception of lightness, and the <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> components are color channels.</p>
                <div class="figure" style="text-align:center">
                <img src="hsv.jpg" alt="HSV Color" style="width:75.0%" />
                <p class="caption">HSV color.</p>
                </div>
                <div class="figure" style="text-align:center">
                <img src="lab.png" alt="Lab Color" style="width:40.0%" />
                <p class="caption">Lab color.</p>
                </div>
                <p>To calculate color similarity, we simply take take the HSV/<span class="math inline">\(Lab\)</span> color information from each pixel and create a histogram for each channel. We then find the histogram intersection.</p>
                <div class="figure" style="text-align:center">
                <img src="histogram.png" alt="The intersection of two histograms is the sum of the heights of the minimum bars." style="width:70.0%" />
                <p class="caption">An illustration of an RGB color histogram for an image. I couldn't find an illustration for an HSV or Lab color histogram.</p>
                </div>
                <div class="figure" style="text-align:center">
                <img src="intersection.png" alt="The intersection of two histograms is the sum of the heights of the minimum bars." style="width:70.0%" />
                <p class="caption">The intersection of two histograms is the sum of the heights of the minimum bars.</p>
                </div>
                <p>The color similarity <span class="math inline">\(s_{color}\)</span> between regions <span class="math inline">\(r_i\)</span> and <span class="math inline">\(r_j\)</span> for a histogram with <span class="math inline">\(n\)</span> bins over <span class="math inline">\(m\)</span> color channels is:</p>
                <p><span class="math display">\[s_{color}(r_i, r_j) = \sum_{c=1}^m\sum_{k=1}^n \min(c_i^k, c_j^k)\]</span></p>
                <p>Here, <span class="math inline">\(c_i^k\)</span> is the height of the <span class="math inline">\(k\)</span>th bar in the histogram of the color channel <span class="math inline">\(c\)</span> of the region <span class="math inline">\(i\)</span>.</p>
                <p>The next similarity metric is texture. We can create a map of edges by taking Gaussian derivatives of an image. You can think of this like the Canny edge detection algorithm.</p>
                <p>We know from basic computer vision that we blur an image using Gaussian smoothing. This is generated by convolving an image with a kernel of Gaussian values, i.e. a kernel with the values</p>
                <p><span class="math display">\[G(x,y) = \frac{1}{2\pi\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}\]</span></p>
                <p>However, by taking the partial derivatives of the Gaussian, we can separate this into horizontal and vertical edges:</p>
                <p style="text-align:center"><img src="gaussian.PNG" alt="image" width="80%"/></p>
                <p>Using a smoothing filter, we remove “high-frequency&quot; components. The value of a smoothing filter can never be negative. On the other hand, derivative filters give high absolute values at points of high contrast.</p>
                <div class="figure" style="text-align:center">
                <img src="gaussianedge.PNG" alt="Note how the x-derivative generates vertical edges, and the y-derivative generates horizontal edges." style="width:80.0%" />
                <p class="caption">Note how the x-derivative generates vertical edges, and the y-derivative generates horizontal edges.</p>
                </div>
                <p>Selective Search calculates Gaussian derivatives in eight directions for <em>each</em> color channel. Shown below are four Gaussian derivatives. Then, for each Gaussian derivative, we create a histogram with 10 bins. We of course find the histogram intersection using a very similar formula to the one for color similarity.</p>
                <div class="figure" style="text-align:center">
                <img src="derivatives.PNG" alt="Note how the x-derivative generates vertical edges, and the y-derivative generates horizontal edges." style="width:40.0%" />
                <p class="caption">Four Gaussian derivatives.</p>
                </div>
                <p>Hence, the texture similarity <span class="math inline">\(s_{texture}\)</span> between regions <span class="math inline">\(r_i\)</span> and <span class="math inline">\(r_j\)</span> for a histogram with <span class="math inline">\(n\)</span> bins over <span class="math inline">\(m\)</span> color channels, each with <span class="math inline">\(p\)</span> Gaussian derivatives is:</p>
                <p><span class="math display">\[s_{texture}(r_i, r_j) = \sum_{g=1}^p\sum_{c=1}^m\sum_{k=1}^n \min(g_{c_i}^k, g_{c_j}^k)\]</span></p>
                <p>Here, <span class="math inline">\(g_{c_i}^k\)</span> is the height of the <span class="math inline">\(k\)</span>th bar in the histogram of the Gaussian derivative <span class="math inline">\(g\)</span> of the color channel <span class="math inline">\(c\)</span> of the region <span class="math inline">\(i\)</span>.</p>
                <p>Size similarity is trivial. We wish to combine smaller regions into larger ones; to increase our similarity metric between small regions, we simply set the size similarity <span class="math inline">\(s_{size}\)</span> of two regions <span class="math inline">\(r_i\)</span> and <span class="math inline">\(r_j\)</span> to <span class="math display">\[s_{size}(r_i, r_j) = 1 - \frac{size(r_i) + size(r_j)}{size(image)}\]</span> Lastly, we have shape compatibility. We wish for our merged regions to fit together cohesively. Again, the formula is decidedly trivial. We simply take the difference between the size of the bounding box <span class="math inline">\(BB\)</span> and the sum of the region areas. Then, we divide this by the size of the image (to normalize the difference to a maximum of 1), and subtract from 1 so that smaller differences have greater similarity.</p>
                <p><span class="math display">\[s_{fill}(r_i, r_j) = 1 - \frac{size(BB_{ij}) - size(r_i) - size(r_j)}{size(image)}\]</span></p>
                <p style="text-align:center"><img src="bb1.PNG" alt="image" style="width:50.0%" /><img src="bb2.PNG" alt="image" style="width:50.0%" /></p>
                <p>Finally, we arrive at our overall similarity metric <span class="math inline">\(s\)</span>, a linear combination for the four sub-metrics. In the formula, <span class="math inline">\(a_1\)</span>, <span class="math inline">\(a_2\)</span>, <span class="math inline">\(a_3\)</span>, and <span class="math inline">\(a_4\)</span> are constants.</p>
                <p><span class="math display">\[s(r_i, r_j) = a_1s_{color}(r_i, r_j) + a_2s_{texture}(r_i, r_j) + a_3s_{size}(r_i, r_j) + a_4s_{fill}(r_i, r_j)\]</span></p>
                <h3 id="evaluation">Evaluation</h3>
                <p>We can measure the accuracy of Selective Search using the Average Best Overlap metric. ABO is the average across all images (and objects) of the area overlap of the best selected box and ground truth box. In the accuracy tables below, the reported metric MABO refers to Mean Average Best Overlap.</p>
                <p style="text-align:center"><img src="abo.PNG" alt="image" style="width:50.0%" /></p>
                <p style="text-align:center"><img src="results.PNG" alt="image" style="width:100.0%" /></p>
                <p>Selective Search not only works best when all four similarity metrics are used (see table above), but also significantly outperformed all other methods at the time of publication (below). Note the inclusion of the recall metric below. Note that Selective Search “Quality&quot; is simply a slower, but more accurate, ensemble method of Selective Search, and the R-CNN paper uses the “Fast&quot; version of Selective Search we have detailed above.</p>
                <p style="text-align:center"><img src="performance.PNG" alt="image" /></p>
                <p>Now that we’ve taken this rather in-depth detour into the inner workings of Selective Search, let us return to R-CNN with our thorough understanding of its region proposal mechanism.</p>
                <h2 id="the-cnn">The CNN</h2>
                <p>The original R-CNN paper used AlexNet as the CNN backbone, but any large CNN (pre-trained on ImageNet) works well. Note that ImageNet only has standard image-level annotations, not object-level annotations. Nevertheless, pre-training greatly improves performance. However, since ImageNet has 1000 classes, after pre-trainining, we replace the 1000-way classification layer with a layer of length <span class="math inline">\(N+1\)</span>, where <span class="math inline">\(N\)</span> is the number of object classes (the <span class="math inline">\(+1\)</span> is for the “background&quot; class).</p>
                <p style="text-align:center"><img src="warp.PNG" alt="image" /></p>
                <p>Selective Search proposes  2000 regions (also called Regions of Interest or RoIs) that are then run through the CNN. Since a CNN requires a fixed input size, and our region proposals can be any size or shape rectangle, we first warp the region to the input size (for AlexNet <span class="math inline">\(227\times227\)</span>). We then fine-tune the CNN to our domain (i.e. object detection). When training the CNN, we treat all regions with <span class="math inline">\(\geq 0.5\)</span> IoU overlap between the predicted and ground-truth boxes as positive samples, and the rest as negatives for all classes (background).</p>
                <p style="text-align:center"><img src="iou_stop_sign.jpg" alt="image" style="width:50.0%" /><img src="iou_equation.png" alt="image" style="width:50.0%" /></p>
                <h2 id="object-category-classifiers">Object Category Classifiers</h2>
                <p style="text-align:center"><img src="rcnn2.png" alt="image"/></p>
                <p>Now that we’ve covered the region proposal and CNN parts of R-CNN, we can discuss training the binary SVM classifiers. We define positive examples as only the ground truth boxes. We set regions with an IoU overlap <span class="math inline">\(\leq 0.3\)</span> as negative examples for that class. Regions with <span class="math inline">\(0.3&lt;IoU&lt;1\)</span> are ignored. One linear SVM is optimized per class.</p>
                <p>It is reasonable to ask: Why use SVMs at all? Why not just use a softmax layer of size <span class="math inline">\(N+1\)</span> as the last layer of the Convolutional network, and use that to classify objects? In addition, why is the IoU threshold different for the SVM training than the IoU threshold for CNN fine-tuning?</p>
                <p>Let’s first consider the IoU threshold question. Because we cannot prove anything in machine learning (rather, we can only prove very limited results), the authors can only hypothesize <em>why</em> this configuration of IoU thresholds works the best. If the authors expand the definition of positive examples for the SVM to include any regions with IoU <span class="math inline">\(\geq 0.5\)</span>, the number of positive examples increases thirty-fold. The authors hypothesize that this larger positive dataset is useful when fine-tuning the entire network (i.e. the CNN), but because it does not teach the network precise localization, accuracy declines when using this looser definition of positive for SVM training.</p>
                <p>The <span class="math inline">\(0.3\)</span> threshold for defining negative examples for the SVM was found through experimentation. Using <span class="math inline">\(0.5\)</span> or <span class="math inline">\(0\)</span> decreased performance dramatically (mAP decreased 5 and 4 points, respectively).</p>
                <p>Next, let’s address the softmax question. The simple answer is that the authors tried using an <span class="math inline">\(N+1\)</span>-way softmax classifier, and mAP decreased <span class="math inline">\(3.3\)</span> points. Again, we cannot prove <em>why</em> softmax causes a decrease in mean average precision, but the authors hypothesize that the definition of positive examples (which we discuss in the above paragraphs) was one of the contributing factors.</p>
                <h2 id="bounding-box-regression">Bounding-box Regression</h2>
                <p>You may have noticed the second “head&quot; of the network labeled “Bbox reg&quot; in the above diagram. Bounding-box regression improves localization performance. We train the bounding-box regression using the input-label pairs <span class="math inline">\(\{(P^i, G^i)\}_{i=1,\dots,N}\)</span> where <span class="math inline">\(P^i = (P^i_x, P^i_y, P^i_w, P^i_h)\)</span>. <span class="math inline">\(P_x\)</span> is the x-coordinate center of the predicted bounding box, <span class="math inline">\(P_y\)</span> is the y-coordinate center of the predicted bounding box, <span class="math inline">\(P_w\)</span> and <span class="math inline">\(P_h\)</span> are the width and height of the predicted bounding box respectively. <span class="math inline">\(G^i = (G^i_x, G^i_y, G^i_w, G^i_h)\)</span>, and refers to the above parameters of ground-truth box. We have four parameters to transform <span class="math inline">\(P\)</span> into <span class="math inline">\(G\)</span>. <span class="math inline">\(d_x(P)\)</span> and <span class="math inline">\(d_y(P)\)</span> translate the center coordinates, and <span class="math inline">\(d_w(P)\)</span> and <span class="math inline">\(d_h(P)\)</span> transform the width and height.</p>
                <p>The specifics of the regression math are unimportant for our study, rather, an understanding of how different predicted boxes affect the regression is more useful. Know that not only do we wish for our predicted center to match the ground truth center, but also the predicted box height and width to match the ground truth. The coordinates in the figure below correspond to <span class="math inline">\((d_x, d_y, d_w, d_h)\)</span>.</p>
                <p style="text-align:center"><img src="bbox.PNG" alt="image" /></p>
                <h2 id="performance">Performance</h2>
                <p>R-CNN outperformed previous methods on object detection tasks.</p>
                <p style="text-align:center"><img src="rcnnresults.PNG" alt="image" /></p>
                <p>However, there are a few caveats. First, R-CNN is slow. Using a VGG16 backbone, R-CNN was reported to take 47 seconds per image. In addition, training is also slow (84 hours reported). Think about it—we have to run the Convolutional network for each of our 2000 proposed regions! Finally, there are issues with the network itself. Training is a multi-stage complex pipeline, with post-hoc linear SVM and bounding-box regression training. Luckily, many of these problems were solved with Fast R-CNN.</p>
                <h1 id="fast-r-cnn">Fast R-CNN</h1>
                <p>Fast R-CNN improves upon R-CNN by running the entire image through the convolutional network, rather than proposing regions first. As a reminder, a feature map is simply the output of a filter applied to the previous layer. A filter is another word for a kernel, which you should remember from our lecture on CNNs.</p>
                <p style="text-align:center"><img src="fastrcnn.PNG" alt="image" /></p>
                <p>Now, instead of running the CNN 2,000 times, we can just run it once. Instead of taking crops from the image, we instead take crops from the convolutional feature map corresponding to the regions of interest in the image.</p>
                <h2 id="roi-projection">RoI Projection</h2>
                <p>The diagram below makes it clear that we propose regions of interest from the input image. These are generated using a region proposal algorithm (e.g. Selective Search), just like they were in R-CNN.</p>
                <p style="text-align:center"><img src="fast-rcnn-arch.png" alt="image" /></p>
                <p>However, in Fast R-CNN, as the first diagram shows, these regions of interest are then projected onto the convolutional feature map. This allows us to re-use the expensive convolutional computation. We take crops from this feature map and run them through the rest of the network. The question then becomes: how exactly do we <em>project</em> a region of the input image onto a region of the convolutional feature map?</p>
                <p>The SPPNet paper (“Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&quot; by He et al.), which came out between R-CNN and Fast R-CNN, first introduced the idea of RoI projection. In the SPPNet paper, the authors pad each layer with filter size <span class="math inline">\(F\)</span> by <span class="math inline">\(&lceil; \dfrac{F}{2} &rceil;\)</span> pixels.</p>
                <p>If we remember from the CNN quiz, we know that a Convolutional layer with input volume <span class="math inline">\(W_1 \times H_1 \times D_1\)</span>, number of filters <span class="math inline">\(K\)</span>, filter size <span class="math inline">\(F\)</span>, stride length <span class="math inline">\(S\)</span>, and amount of zero padding <span class="math inline">\(P\)</span> has output volume of size <span class="math inline">\(W_2 \times H_2 \times D_2\)</span>. <span class="math inline">\(W_2\)</span>, <span class="math inline">\(H_2\)</span>, and <span class="math inline">\(D_2\)</span> can be calculated using the following formulas:</p>
                <p><span class="math display">\[W_2=\dfrac{W_1+2P-F}{S} + 1\]</span> <span class="math display">\[H_2=\dfrac{H_1+2P-F}{S} + 1\]</span> <span class="math display">\[D_2=K\]</span></p>
                <p>Substituting in <span class="math inline">\(P\)</span> for <span class="math inline">\(\dfrac{F}{2}\)</span>, we find for even <span class="math inline">\(F\)</span>: <span class="math display">\[W_2=\dfrac{W_1}{S} + 1\]</span> <span class="math display">\[H_2=\dfrac{H_1}{S} + 1\]</span></p>
                <p>and for odd <span class="math inline">\(F\)</span>: <span class="math display">\[W_2=\dfrac{W_1-1}{S} + 1\]</span> <span class="math display">\[H_2=\dfrac{H_1-1}{S} + 1\]</span></p>
                <p>Thus, the SPPNet paper states that for a region projected onto a feature map with central coordinates <span class="math inline">\((x&#39;,y&#39;)\)</span>, the center of the region on the image is:</p>
                <p><span class="math display">\[(x,y) = (S&#39;x&#39;, S&#39;y&#39;)\]</span></p>
                <p>Where <span class="math inline">\(S&#39;\)</span> is the product of all previous stride lengths (Remember, there are multiple pooling and convolutional layers between the input image and the final feature map). Finally, given the coordinates for the left, right, top, and bottom of the input region <span class="math inline">\(x_L\)</span>, <span class="math inline">\(x_R\)</span>, <span class="math inline">\(y_T\)</span>, <span class="math inline">\(y_B\)</span> respectively, we find the coordinates of the receptive field in the filter map:</p>
                <p><span class="math display">\[x&#39;_L = &lceil; \frac{x_L}{S&#39;} &rceil; +1\]</span> <span class="math display">\[x&#39;_R = &lfloor; \frac{x_R}{S&#39;} &rfloor; -1\]</span> <span class="math display">\[y&#39;_T = &lceil; \frac{y_T}{S&#39;} &rceil; +1\]</span> <span class="math display">\[y&#39;_R = &lfloor; \frac{y_B}{S&#39;} &rfloor; -1\]</span></p>
                <p>The proof for these formulas, along with the <span class="math inline">\((x,y)\)</span> formula above, are left as exercises for the reader. Note that this math is not overly important, as the entire RoI projection is done away with for Faster R-CNN.</p>
                <p>The authors of the SPPNet paper leaves us with a final caveat: If the padding is not <span class="math inline">\(&lceil; \dfrac{F}{2} &rceil;\)</span>, then “we need to add a proper offset&quot;, but the authors give no calculation for such offset.</p>
                <h2 id="roi-pooling">RoI Pooling</h2>
                <p>We need to run our RoI projections (crops of the convolutional feature map that correspond to regions of interest in the input image) through fully connected layers. However, these fully connected layers demand a fixed input size. We must have a way to convert differently-sized regions into a standard size. In R-CNN, we solved this problem by simply warping parts of the RoI. However, that was when our regions were direct crops of the image, and region warping was not part of a network being trained. We need a better technique, one that is differentiable, so that the entire network is trainable. Enter RoI Pooling.</p>
                <p>RoI Pooling is similar to max-pooling, which we covered in our first CNN lecture. Let’s say our convolutional feature map has size <span class="math inline">\(8\times8\)</span>, and our first fully connected layer needs an input of size <span class="math inline">\(2\times2\)</span>. RoI Pooling would then follow the figures below.</p>
                <p style="text-align:center"><img src="roipool1.jpg" alt="image" style="width:80.0%" /></p>
                <p style="text-align:center"><img src="roipool2.jpg" alt="image" style="width:80.0%" /></p>
                <div class="figure" style="text-align:center">
                <img src="roipool3.jpg" alt="2\times2 output." style="width:80.0%" />
                </div>
                <div class="figure" style="text-align:center">
                <img src="roipool4.jpg" alt="2\times2 output." style="width:20.0%" />
                <p class="caption">RoI Pooling with a <span class="math inline">\(2\times2\)</span> output.</p>
                </div>
                <p>Simply put, if an <span class="math inline">\(N\times N\)</span> output is desired, the proposed region (black rectangle in the 2nd image) is divided into an <span class="math inline">\(N\times N\)</span> grid. When the region dimensions are not divisible by <span class="math inline">\(N\)</span>, the sections of the grid will not all contain the same number of pixels. From each section, we take the greatest pixel value, and arrange these values in an <span class="math inline">\(N\times N\)</span> grid, forming our output. This output is then passed through the fully connected layers.</p>
                <p style="text-align:center"><img src="roipoolinggif.gif" alt="image" style="width:100.0%" /></p>
                <h2 id="training">Training</h2>
                <p>Fast R-CNN is much faster (and easier) to train since the entire network is trainable, end-to-end. Again, we are training two things: a probability discribution for each RoI (via a <span class="math inline">\(K+1\)</span>-way softmax, where <span class="math inline">\(K\)</span> is the number of classes, and the <span class="math inline">\(+1\)</span> is for the background), and bounding-box regression offsets. No more post-hoc SVMs. One might ask: Hold on, you said SVMs worked better than Softmax for R-CNN. Shouldn’t they also work better for Fast R-CNN? Short answer: No. Again, no proof. Only experimental results.</p>
                <table>
                <tbody>
                <tr class="odd">
                <td align="center">Method (VGG16)</td>
                <td align="center">Classifier</td>
                <td align="center">VOC07</td>
                <td align="center"></td>
                </tr>
                <tr class="even">
                <td align="center">[0.5ex] R-CNN</td>
                <td align="center">Post-hoc SVM</td>
                <td align="center">66.0%</td>
                <td align="center"></td>
                </tr>
                <tr class="odd">
                <td align="center">Fast R-CNN</td>
                <td align="center">Post-hoc SVM</td>
                <td align="center">66.8%</td>
                <td align="center"></td>
                </tr>
                <tr class="even">
                <td align="center">Fast R-CNN</td>
                <td align="center">Softmax</td>
                <td align="center">66.9%</td>
                <td align="center"></td>
                </tr>
                </tbody>
            </table><br>
                <div class="figure"  style="text-align:center">
                <img src="trainingfastrcnn.PNG" alt="Blue bars include region proposal algorithm." style="width:100.0%" />
                <p class="caption">Blue bars include region proposal algorithm.</p>
                </div>
                <p>Since we are training two things at once, we use a multi-task loss function. The specifics of the loss function are unimportant for our study. Just know that Fast R-CNN greatly improves upon the speed of R-CNN—so much so that the region proposal algorithm takes the vast majority of time during testing.</p>
                <div class="figure"  style="text-align:center">
                <img src="chart.png" alt="Blue bars include region proposal algorithm." style="width:80.0%" />
                <p class="caption">Blue bars include region proposal algorithm. Red bars do not.</p>
                </div>
                <p>Luckily, Faster R-CNN solves our problem of slow region proposal!</p>
                <h1 id="faster-r-cnn">Faster R-CNN</h1>
                <p>As you can see, computer scientists are great at naming their algorithms.</p>
                <p>Faster R-CNN addresses the region proposal problem. Looking back at the table on Page 2, even the fast EdgeBoxes algorithm for region proposal takes 0.2 seconds. Faster R-CNN reduces region proposal to 10 milliseconds (0.01 seconds) through a Region Proposal Network (RPN).</p>
                <h2 id="region-proposal-networks">Region Proposal Networks</h2>
                <p>Selective search is a fixed function. It doesn’t learn. Why not use the information gained from the convolutional network to predict region proposals? In other words, why not predict region proposals from the the very same filter map output we used for RoI projection earlier? Remember, in Fast R-CNN, we realized that we didn’t have to run the convolutional network 2,000 times for our 2,000 regions of interest. Rather, we used the convolutional information, just running the network once, and predicting the regions of interest from a filter map. Faster R-CNN uses that very same filter map to predict region proposals. After all, if convolutions are good enough for classification and bounding-box regression, why wouldn’t they work for region proposals as well?</p>
                <div class="figure"style="text-align:center">
                <img src="fasterrcnntraining.png" alt="Faster R-CNN architecture." style="width:100.0%" />
                <p class="caption">Faster R-CNN architecture.</p>
                </div>
                <p>The Faster R-CNN architecture is shown above. Note that the only difference between the replacement of the RoI projection with the Region Proposal Network (RPN). The upstream architecture (fully connected layers, softmax classifier, and bounding-box regressors) remain unchanged.</p>
                <p>So what exactly is a Region Proposal Network (RPN)? Simply put, it is a small fully convolutional network that takes in a feature map and outputs a set of regions and an “objectness&quot; score for each region (how likely the region is to contain an object).</p>
                <p>How exactly does an RPN use a convolution to create proposed regions? Refer to the diagram below left. The “grid&quot; below is the convolutional feature map (RPN input), and we convolve the input with an <span class="math inline">\(n\times n\)</span> sliding window (kernel).</p>
                <p>In the paper, the authors use a <span class="math inline">\(3\times3\)</span> sliding window. However, since this is on a high-level feature map (which is much smaller than the input), projected on the input image, <span class="math inline">\(3\times3\)</span> corresponds to a much larger receptive field. Rather than simply classifying “object or no object&quot; and regressing the bounding box coordinates for the sliding window at each position, the authors use anchor boxes to allow for regions of different shapes. At each sliding window position, we generate <span class="math inline">\(k=9\)</span> anchor boxes, which have aspect ratios <span class="math inline">\(1:1\)</span>, <span class="math inline">\(2:1\)</span>, and <span class="math inline">\(1:2\)</span>, and size <span class="math inline">\(64\)</span> pixels, <span class="math inline">\(128\)</span> pixels, and <span class="math inline">\(256\)</span> pixels, and are centered at the center of the sliding window. These anchor boxes are <em>translation invariant</em>, meaning they remain the same dimensions throughout the feature map.</p>
                <p style="text-align:center"><img src="anchorboxes.PNG" alt="image" /></p>
                <div class="figure"style="text-align:center">
                <img src="anchors.png" alt="Example anchor boxes of the Region Proposal Network." style="width:70.0%" />
                <p class="caption">Example anchor boxes of the Region Proposal Network.</p>
                </div>
                <p>For a feature map of width <span class="math inline">\(W\)</span> and height <span class="math inline">\(H\)</span> (for which the authors claim <span class="math inline">\(W\times H\)</span> to be <span class="math inline">\(\approx 2400\)</span> pixels total, or about <span class="math inline">\(50\)</span> pixels in either dimension), there are <span class="math inline">\(WHk\)</span> anchors. This implies a padding <span class="math inline">\(P=1\)</span>.</p>
                <p>Each sliding window is mapped to a lower-dimension feature (256 or 512 dimensional, depending on the base CNN). There are two <span class="math inline">\(1\times1\)</span> convolutional layers, one for classification, the other for regression. View our “Fully Convolutional Networks&quot; lecture for more information on <span class="math inline">\(1\times1\)</span> convolutions.</p>
                <p>At each sliding window location, for the <span class="math inline">\(k\)</span> anchor boxes, we predict <span class="math inline">\(2k\)</span> classification scores and <span class="math inline">\(4k\)</span> regression coordinates. The classification is simply “object or no object&quot;, and is implemented as a 2-way softmax. The <span class="math inline">\(4k\)</span> outputs are obviously the coordinates of the box. This is similar to the bounding-box regression we performed in R-CNN, Fast R-CNN (and also in Faster R-CNN).</p>
                <h2 id="training-1">Training</h2>
                <h3 id="training-the-rpn">Training the RPN</h3>
                <p>How do we train the region proposal network? Surely we know the ground truth box, but how to we determine whether an anchor box is close enough to the ground truth box to be counted as positive?</p>
                <p>We use Intersection over Union (IoU), of course! If the IoU between the anchor box and the ground truth box is <span class="math inline">\(&gt;0.7\)</span>, <em>or</em> a particular anchor box has the highest IoU with the ground truth box, then it is counted as a positive example. There may be multiple positive examples per ground truth box. If the IoU between an anchor box and the ground truth box is <span class="math inline">\(&lt;0.3\)</span>, it is assigned a negative label.</p>
                <p>We can now define our loss function for the RPN.</p>
                <p><span class="math display">\[L(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}}\sum_iL_{cls}(p_i,p_i^*) + \lambda\frac{1}{N_{reg}}\sum_ip_i^*L_{reg}(t_i,t_i^*)\]</span></p>
                <p><span class="math inline">\(\{p\}\)</span> is the set of anchor object probabilities in the mini-batch, and <span class="math inline">\(\{t\}\)</span> is our set of predicted anchor coordinates. (Note that these are the outputs of the classification softmax and the regression, respectively).</p>
                <p>Thus, <span class="math inline">\(p_i\)</span> represents the probability that the anchor <span class="math inline">\(i\)</span> is an object. Note that <span class="math inline">\(p_i\)</span>, being the output of the 2-way softmax, is really two numbers, one of which represents the probability <span class="math inline">\(i\)</span> is an object, the other the probability <span class="math inline">\(i\)</span> is not an object, which sum to one. <span class="math inline">\(p_i^*\)</span> is the ground truth (<span class="math inline">\(0\)</span> if anchor <span class="math inline">\(i\)</span> is not an object (<span class="math inline">\(IoU &lt; 0.3\)</span>), and <span class="math inline">\(1\)</span> if <span class="math inline">\(i\)</span> is an object (<span class="math inline">\(IoU &gt; 0.7\)</span>)). <span class="math inline">\(L_{cls}(p_i,p_i^*)\)</span> is simply a multi-class log loss (log loss over the two classes, object and not object).</p>
                <p>Similarly, <span class="math inline">\(t_i\)</span> is the 4 coordinates of the bounding box (x, y, width, height), and <span class="math inline">\(t_i^*\)</span> is the coordinates of the ground-truth box. <span class="math inline">\(L_{reg}(t_i,t_i^*)\)</span> is the regression function, which we said in our R-CNN section about bounding-box regression that we would not go into the mathematics of. Note that the regression is only applied when we are working with an anchor box that contains an object (<span class="math inline">\(p_i^*=1\)</span>). When <span class="math inline">\(p_i^*=0\)</span>, <span class="math inline">\(L_{reg}=0\)</span>.</p>
                <p><span class="math inline">\(N_{cls}\)</span> and <span class="math inline">\(N_{reg}\)</span> are normalization terms, and correspond to the mini-batch size and the number of anchors, respectively. Thus, in the original paper, <span class="math inline">\(N_{cls} = 256\)</span>, and <span class="math inline">\(N_{reg} \approx 2400\)</span>. <span class="math inline">\(\lambda\)</span> is used to weight the two, and in practice <span class="math inline">\(\lambda=10\)</span> to weight the terms approximately evenly.</p>
                <p>One might ask: How do we determine the <span class="math inline">\(256\)</span> anchors in our mini-batch? Clearly, there are far more than 256 anchors (<span class="math inline">\(WHk \approx 21600\)</span>). In the Faster R-CNN paper, the authors randomly sample <span class="math inline">\(256\)</span> anchors. However, there are obviously <em>far</em> more negative anchor boxes than positive anchor boxes. Thus, the authors ensure that if there are <span class="math inline">\(n\)</span> positive anchor boxes, the mini-batch contains <span class="math inline">\(\max(n, 128)\)</span> positive anchor boxes.</p>
                <h3 id="training-faster-r-cnn">Training Faster R-CNN</h3>
                <p>The rest of Faster R-CNN is identical to Fast R-CNN. The RPN is the real innovation of Faster R-CNN, however, because of the RPN, the authors had to tweak the relatively simple Faster R-CNN “end-to-end&quot; training pipeline. In the original paper, the authors present a complex 4-step training pipeline.</p>
                <ul>
                <li><p>Step 0: The base CNN is pre-trained on ImageNet.</p></li>
                <li><p>Step 1: Train the RPN (as described in the section above).</p></li>
                <li><p>Step 2: Train Fast R-CNN indepndently from the RPN, using the generated proposals from the step-1-trained RPN (Of course, this Fast R-CNN is also using a pre-trained ImageNet network).</p></li>
                <li><p>Step 3: Fix the shared convolutional layers, and fine-tune the RPN layers.</p></li>
                <li><p>Step 4: Fine tune the unique Fast R-CNN layers.</p></li>
                </ul>
                <p>Obviously, we are glossing over the details, especially with “fix the shared convolution layers&quot;, although I should mention little detail is given in the paper itself on this.</p>
                <p>Since then, much simpler, faster, joint training has been developed. Essentially, we train the entire Faster R-CNN model at once, and now instead of having a two-task loss, we have a four-task loss, as we learn region proposals, region proposal coordinates, bounding-box coordinates, and object classification at once. We will not go into the specifics of the loss function for this joint training.</p>
                <p>There are a few more details for Faster R-CNN training. First, all images are re-scaled so that the shorter side <span class="math inline">\(s=600\)</span> pixels. Then, there is the question of which of the roughly <span class="math inline">\(60\times40\times9=21600\)</span> anchor boxes per image to include during training. Ignoring anchor boxes that cross the image boundaries, we are left with <span class="math inline">\(\approx 6000\)</span> anchor boxes. Many of these are highly redundant, so we apply non-maximum suppression on proposed regions to leave us with about 2000 per image.</p>
                <h2 id="non-maximum-suppression">Non-Maximum Suppression</h2>
                <p>What exactly is non-maximum suppression (NMS), and how does it help us reduce the number of redundant (highly overlapping) region proposals?</p>
                <p>Non-Maximum Supression is trivial. For bounding boxes with an IoU over a certain threshold (in this case, the threshold is <span class="math inline">\(IoU=0.7\)</span>), pick the one with the greatest score, and get rid of the rest. In this case, the score is the IoU of a bounding box with the ground truth box. It is important to realize that the <span class="math inline">\(0.7\)</span> refers to the threshold of overlap of predicted boxes with each other, not with the ground truth box.</p>
                <p>In other words, if you are an anchor box, and you have <span class="math inline">\(IoU&gt;0.7\)</span> with another anchor box, and that anchor box has a higher IoU with the ground truth box than you do, R.I.P. you. Ya gone.</p>
                <p>If you are still unclear, it’s often easiest to understand through simple code. You can view a Python implementation <a href="https://github.com/vmalpani/NonMaximalSuppression/blob/master/non_maximal_suppression.py">here</a>.</p>
                <h2 id="model-performance">Model Performance</h2>
                <p>Faster R-CNN not only dramatically improves the speed of Fast R-CNN, it also outperforms Fast R-CNN with Selective Search region proposals.</p>
                <p style="text-align:center"><img src="fasterresults.PNG" alt="image" /></p>
                <p>Not only does Faster R-CNN reduce testing time due to the elimination of the selective search bottleneck (2 seconds down to 10 milliseconds), it also reduces the number of region proposals, so fewer regions need to be propagated through the RoI pooling and fully connected layers.</p>
                <p>See, although we used roughly 2000 region proposals per image when training, the RPN allows us to use far less during test-time (speeding up the network). The authors are able to get away with a maximum of 300 region proposals; sometimes NMS eliminates so many region proposals that the network is left with fewer than 300. In fact, the RPN’s top-ranked region proposals are so accurate that using only 100 proposals at test-time does not decrease accuracy significantly.</p>
                <p style="text-align:center"><img src="fastercoco.PNG" alt="image" /></p>
                <p>The end result is a model that runs in frames per second (FPS), rather than seconds per frame, like R-CNN and Fast R-CNN. Note that in the tables above and below, ZF is just another large network pre-trained on ImageNet. VGG performs better than ZF, but is more complex, and hence runs slower, as shown below. One achieves even better results than the ones above when using a newer model pre-trained on ImageNet, like ResNet or Inception-ResNet.</p>
                <p style="text-align:center"><img src="fasterspeed.PNG" alt="image" /></p>
                <p>One might ask: How do anchor box configuration and <span class="math inline">\(\lambda\)</span> values affect performance? Faster R-CNN is surprisingly resilient to hyperparameter shifts, as shown below. It turns out, changes in anchor ratios don’t affect performance heavily, and neither do small <span class="math inline">\(\lambda\)</span> changes.</p>
                <p style="text-align:center"><img src="anchorperformance.PNG" alt="image" width="60%"/></p>
                <p style="text-align:center"><img src="lambdaperformance.PNG" alt="image" width="60%"/></p>
                <p>We’ve now covered the R-CNN family of networks. These networks all have the same basic structure, and use a large CNN pre-trained on ImageNet. They view object detection as largely a classification problem, with regression heads to optimize bounding-boxes. The next network we cover, the Single Shot MultiBox Detector (SSD), views object detection differently.</p>
                <h1 id="single-shot-multibox-detector">Single Shot MultiBox Detector</h1>
                <p>In the field of computer science, the acronym SSD has a popular meaning. The Solid-state drive has been replacing traditional hard drives in mainstream consumer computer hardware for the better part of a decade, but nevertheless, the authors of the “Single Shot MultiBox Detector&quot; went with the acronym for their algorithm. Thus, in the field of machine learning, SSD refers to a blazing fast object detection network.</p>
                <p>SSD is actually much simpler than Faster R-CNN. It is this simplicity, the elimination of many structures from Faster R-CNN, that allows it near real-time speed.</p>
                <p>In Faster R-CNN, we perform two discrete steps in order to detect objects. First, we run a region proposal algorithm. Then, we classify the regions. SSD combines this into one step by getting rid of the separate region classification step. In essence, SSD takes a regression approach to object detection.</p>
                <p>Think back to the Region Proposal Network (RPN) we discussed in Faster R-CNN. Our small fully convolutional network outputted, for each anchor box, classification scores (“object vs. not object&quot;) and regression scores to correct coordinates. What if, instead of classifying “object vs. not object&quot;, we just outputted a predicted object class (or background) for the proposed region? This is the approach the Single Shot Detector takes.</p>
                <h2 id="related-work-yolo">Related Work: YOLO</h2>
                <p>SSD is not the only network to takes this “Single Shot&quot; approach. An earlier network, called YOLO (for <em>You Only Look Once</em>), took a similar approach to object detection. We will provide a high-level overview of YOLO, however, it is less important than SSD because SSD achieves much better results than YOLO.</p>
                <p style="text-align:center"><img src="yolo.PNG" alt="image" /></p>
                <p>YOLO divides an image into <span class="math inline">\(7\times7\)</span> grid, and then for each grid cell proposes <span class="math inline">\(B\)</span> bounding boxes (think anchor boxes from Faster R-CNN) for regions. For each bounding box we output the four coordinates, as well as a confidence score. This confidence score is defined as: <span class="math display">\[C = \text{Probability}(\text{Object}) \cdot \text{IoU}(\text{truth}, \text{box})\]</span></p>
                <p>Obviously, we want our Confidence <span class="math inline">\(C\)</span> to be <span class="math inline">\(0\)</span> if there is no object in the cell (and thus IoU<span class="math inline">\(=0\)</span>). Otherwise, we want <span class="math inline">\(C\)</span> to be the IoU(truth, box), since the probability of an object should be as close to <span class="math inline">\(1\)</span> as possible.</p>
                <p>In addition, for each grid cell, we predict the probability for each class. We multiply grid cell class probabilities by bounding box confidence, and thus get the class scores for each of the cell’s bounding boxes. Finally, we use non-maximum suppression to get rid of extraneous boxes.</p>
                <p>Like all the other object detection networks, YOLO is a deep convolutional network pre-trained on ImageNet. YOLO then adds a few convolutional layers and is trained using a multi-task loss function. We will skip over the details of this; it is only important to know that although YOLO achieves a high framerate (45 FPS vs. Faster R-CNN’s 7 FPS), it is <em>less accurate than Faster R-CNN by 7 mAP</em>.</p>
                <p style="text-align:center"><img src="yologrid.PNG" alt="image" /></p>
                <p>YOLOv2 achieves SSD-level performance (but faster) by adding batch normalization, using k-means to optimize anchor boxes, changing network size during training, and using a base CNN with fewer parameters.</p>
                <div class="figure" style="text-align:center">
                <img src="yolo2coco.PNG" alt="COCO results." style="width:100.0%" />
                <p class="caption">YOLOv2 COCO results.</p>
                </div>
                <div class="figure" style="text-align:center">
                <img src="yolo2pascal.PNG" alt="PASCAL results." style="width:60.0%" />
                <p class="caption">YOLOv2 PASCAL results.</p>
                </div>
                <h2 id="network-architecture">Network Architecture</h2>
                <p style="text-align:center"><img src="ssdonly.PNG" alt="image" /></p>
                <p>The SSD architecture (view diagram above) is simply composed of a standard large CNN pre-trained on ImageNet, followed by convolutional layers of varying size. The network outputs bounding boxes along with a class prediction for each output.</p>
                <p>VGG16 serves as the “base network&quot; of SSD in the original paper; the last two fully connected layers (FC6 and FC7) are converted into convolutional layers (Conv6 and Conv7), as shown in the diagram above. See our Fully Convolutional Networks lecture for more information on converting fully connected layers to convolutional layers.</p>
                <p>The real magic of the SSD lies in the convolutional layers of varying size following the base network. See, SSD proposes default boxes at every cell for each of the added feature layers (the convolutional layers of varying sizes). However, these default boxes (think anchor boxes from RPN) increase as the cell size of the feature maps increases. In other words, as the diagram below suggests, earlier feature maps have more pixels, and thus anchor boxes are smaller, while in later feature maps, anchor boxes are larger. This allows us to detect objects of widely varying sizes. The aspect ratios of the anchor boxes are consistent throughout; we use the same set of 6 aspect ratios throughout all feature maps.</p>
                <p>Think back to the skip connections from the Fully Convolutional Networks (FCN) lecture. Remember how skip connections combined and upscaled smaller feature maps with finer feature maps to improve fine semantic segmentation. Using feature maps of different sizes helps similarly here.</p>
                <p style="text-align:center"><img src="ssdmaps.PNG" alt="image" /></p>
                <p>It should be noted that the anchor boxes <em>do not</em> scale linearly with the size of the cell width, but rather, according to the following formula:</p>
                <p><span class="math display">\[s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1}(k-1), k \in [1,m]\]</span></p>
                <p>Where <span class="math inline">\(s_{min}=0.2\)</span> and <span class="math inline">\(s_{max}=0.9\)</span>. <span class="math inline">\(m\)</span> is the number of feature maps we are using for prediction. Thus, the lowest layer has a scale of 0.2, and the highest layer has a scale of 0.9. Thus, on later layers, which have larger cells, we will have larger boxes. For each feature map cell, we have 6 default boxes centered on the center of the cell, with aspect ratios <span class="math inline">\(1:1\)</span>, <span class="math inline">\(1:2\)</span>, <span class="math inline">\(2:1\)</span>, <span class="math inline">\(1:3\)</span>, and <span class="math inline">\(3:1\)</span>. There are two <span class="math inline">\(1:1\)</span> boxes, one slightly larger than the other.</p>
                <p>For each box, we compute <span class="math inline">\(c\)</span> class scores and <span class="math inline">\(4\)</span> offsets relative to the box location. This requires <span class="math inline">\((c+4)k\)</span> filters. We use a <span class="math inline">\(3\times3\)</span> convolutional filter, and generate <span class="math inline">\((c+4)kmn\)</span> outputs for an <span class="math inline">\(m\times n\)</span>-sized feature map.</p>
                <p>After we generate the 8732 detections per class, we use non-maximum suppression to determine our final detections.</p>
                <h2 id="training-2">Training</h2>
                <p>One now asks: how do we determine which default boxes are positive and which are negative for training?</p>
                <p>We use Intersection over Union, of course! I hope you realize by now that this is very similar to the anchor boxes for Fast R-CNN. If a default box has an IoU <span class="math inline">\(&gt; 0.5\)</span> with a ground truth box, or if it has the highest IoU, then it is counted as a positive example. Note that another name for Intersection over Union is jaccard overlap.</p>
                <p>We will not go into the details of the loss function. Simply know that we have (again) a multi-task loss function. We wish to minimize a weighted sum of a smooth L1 localization loss (between the predicted box and ground truth box coordinates) and confidence loss (softmax loss over <span class="math inline">\(c\)</span> classes).</p>
                <p>It should be noted that of course, after matching default boxes to positive or negative, depending on IoU, we end up with far more negative boxes. Thus, the authors sort negative examples by highest confidence loss and choose negative examples so that the ratio of positive to negative training samples is <span class="math inline">\(1:3\)</span>. This technique is called <strong>hard negative mining</strong>, because these negative examples are the closest to positive examples, and are thus the most difficult to classify.</p>
                <p>Data augmentation was found to be a potent mechanism to improve performance. The authors randomly sampled each training image by one of the following methods:</p>
                <ul>
                <li><p>Use original image</p></li>
                <li><p>Sample patch that have a minimum IoU with object boxes of <span class="math inline">\(0.1+0.2k, k \in \{0,1,2,3,4\}\)</span></p></li>
                <li><p>Randomly sample a patch such that patch size <span class="math inline">\(\in [0.1, 1]\)</span> of image size, and patch aspect ratio <span class="math inline">\(\in [\dfrac{1}{2}, 2]\)</span> of image aspect ratio.</p></li>
                </ul>
                <p>In addition to this data augmentation, the authors use an “extensive sampling strategy&quot;, similar to the techniques in the YOLO paper. The YOLO paper used random scaling and translations up to 20% of the original image size. This conflicts with the information in this list above, but I could find no explanation of which method used, so assume both methods were used. In addition, YOLO randomly adjusted image exposure and saturation by a factor of <span class="math inline">\(1.5\)</span> in the HSV color space. For more information on HSV, see the section on Selective search similarity metrics.</p>
                <h2 id="model-performance-1">Model Performance</h2>
                <p>The tables below show the model performance. SSD300 denotes the SSD with input size <span class="math inline">\(300\times300\)</span>, and SSD512 denotes the SSD with input size <span class="math inline">\(512\times512\)</span>. SSD512 performs better in almost every task. The best SSD300 and SSD512 models perform better than Fast R-CNN and Faster R-CNN in every task.</p>
                <p style="text-align:center"><img src="ssdresults.PNG" alt="image" /></p>
                <p>We said before that the novel part of SSD is the convolutional layers of varying sizes that follow the base VGG network, because these layers give us an efficient way to create default boxes of varying sizes (discretizing the output space). The table below shows the performance degradation when some of these layers are removed.</p>
                <p style="text-align:center"><img src="ssdlayers.PNG" alt="image" /></p>
                <p>SSD300 struggles on smaller objects. Even though the authors are able to produce higher mAP than Faster R-CNN with a smaller image size (roughly <span class="math inline">\(1000\times600\)</span> for Faster R-CNN vs. <span class="math inline">\(512\times512\)</span> for SSD512), the authors note that there is considerable room for improvement for detection of small objects. Why does it perform poorly on small objects? Again, we cannot prove the cause, but the authors suggest that small objects will have little information available at top layers (which have much coarser filter maps). Clearly, increasing input size alleviates this issue somewhat. This is shown by the dramatic improvements in average precision and average recall between SSD300 and SSD512 for small objects in the table below.</p>
                <p style="text-align:center"><img src="ssdcoco.PNG" alt="image" /></p>
                <p>The data augmentation procedures explained in the "Training" section above improve SSD performance considerably (8.8% mAP on VOC2007). The authors claim that Faster R-CNN should not see the same improvement due to its feature pooling step robust to object translation.</p>
                <p>The authors took augmentation a step further with SSD300* and SSD512*. By tiling <span class="math inline">\(4\times4\)</span> of the same image and then taking crops, they effectively create a “zoom-out&quot; feature, as opposed to the “zoom-in&quot; data cropping augmentation from before. This expands the dataset and requires twice the training, but improves mAP 2-3%.</p>
                <p style="text-align:center"><img src="ssdaugmentation.PNG" alt="image" /></p>
                <p>SSD300 operates in real-time (near 60 fps). However, as shown by the table below, SSD512, since it is dealing with a larger input size, no longer operates in real-time. Still, it is faster than competing methods (YOLO), while outperforming YOLO considerably. It should be noted that about 80% of the test time is spent on the base network (VGG16), so any improvements to SSD would not have a dramatic effect on overall speed. However, a faster base network would have a larger effect on SSD runtime.</p>
                <p style="text-align:center"><img src="ssdoverall.PNG" alt="image" /></p>
                <h2 id="more-information">More Information</h2>
                <p>If you would like an explanation of an implementation of SSD, or a detailed breakdown of the loss function, I’ve found a three-part series, and the first part is linked <a href="https://towardsdatascience.com/learning-note-single-shot-multibox-detector-with-pytorch-part-1-38185e84bd79">here</a>.</p>
                <h1 id="conclusion">Conclusion</h1>
                <p>Object detection is a difficult task. Although drastic improvements have been made in the past few years, the most effective methods all operate off a large base CNN pretrained on ImageNet. Now that base CNNs like VGG16 take the majority of object detection runtime, the challenge will be to achieve similar results with a smaller base CNN, or to improve accuracy beyond current state-of-the-art models on the PASCAL and COCO benchmarks.</p>
                <h1 id="acknowledgements">Acknowledgements</h1>
                <p>I did not create any of these diagrams. I procured most of these diagrams from the following sources:</p>
                <ul>
                <li><p><a href="https://arxiv.org/pdf/1311.2524.pdf">R-CNN Paper</a></p></li>
                <li><p><a href="http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf">Selective Search Paper</a></p></li>
                <li><p><a href="https://arxiv.org/pdf/1504.08083.pdf">Fast R-CNN Paper</a></p></li>
                <li><p><a href="https://dl.dropboxusercontent.com/s/vlyrkgd8nz8gy5l/fast-rcnn.pdf?dl=0">Ross Girshick’s Fast R-CNN slides (download)</a> <a href="http://www.rossgirshick.info/">[view his website here]</a></p></li>
                <li><p><a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN Paper</a></p></li>
                <li><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf">CS231n Lecture slides</a></p></li>
                <li><p><a href="https://arxiv.org/pdf/1512.02325.pdf">SSD Paper</a></p></li>
                <li><p><a href="https://pjreddie.com/media/files/papers/yolo.pdf">YOLO Paper</a></p></li>
                <li><p><a href="https://arxiv.org/pdf/1612.08242.pdf">YOLOv2 Paper</a></p></li>
                </ul>
            </div>
        <p><a href="../../schedule.html">&larr; Back to lectures</a>
    </section>
  </body>
</html>
